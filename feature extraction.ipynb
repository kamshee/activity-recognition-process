{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question: What's the optimal sensor combo for activity recognition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import platform\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from itertools import product\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import butter, welch, filtfilt\n",
    "\n",
    "# update PreprocessFcns.py with updated functions for use in this notebook\n",
    "#from PreprocessFcns import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#-- For interactive plots--\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refer to Training-Set-Selection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-af185ea880b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mcolnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolnames1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mcolnames2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mDatafinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Data' is not defined"
     ]
    }
   ],
   "source": [
    "# features\n",
    "colnames=['RMSX', 'RMSY', 'RMSZ', 'rangeX', 'rangeY', 'rangeZ', 'meanX',\n",
    "       'meanY', 'meanZ', 'varX', 'varY', 'varZ', 'skewX', 'skewY', 'skewZ',\n",
    "       'kurtX', 'kurtY', 'kurtZ', 'xcor_peakXY', 'xcorr_peakXZ',\n",
    "       'xcorr_peakYZ', 'xcorr_lagXY', 'xcorr_lagXZ', 'xcorr_lagYZ', 'Dom_freq',\n",
    "       'Pdom_rel', 'PSD_mean', 'PSD_std', 'PSD_skew', 'PSD_kur', 'jerk_mean',\n",
    "       'jerk_std', 'jerk_skew', 'jerk_kur', 'Sen_X', 'Sen_Y', 'Sen_Z']\n",
    "\n",
    "\n",
    "sensor_list = ['sacrum'] #['dorsal_hand_']\n",
    "\n",
    "\n",
    "colnames1=[i+'acc' for i in colnames]\n",
    "colnames2=[i+'gyr' for i in colnames]\n",
    "colnames=colnames1+colnames2\n",
    "\n",
    "len(Data)\n",
    "\n",
    "Datafinal = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Data)):\n",
    "    Datatemp=pd.DataFrame()\n",
    "    for sensors in sensor_list:\n",
    "        F1 = Data[sensors + '_accel'].iloc[i]\n",
    "        F2 = Data[sensors + '_gyro'].iloc[i]\n",
    "        F = pd.DataFrame(data=np.hstack((F1,F2)).reshape(-1,1).T,index=[i],columns=[i for i in colnames])\n",
    "        Datatemp=pd.concat((Datatemp,F),axis=1)\n",
    "    Datafinal = pd.concat((Datafinal,Datatemp),axis=0)\n",
    "\n",
    "Datafinal=Datafinal.reset_index(drop=True)\n",
    "# Datanew=Datanew.reset_index(drop=True)\n",
    "Data=Data.reset_index(drop=True)\n",
    "\n",
    "Data = pd.concat((Data.iloc[:,:7],Datafinal),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#drop features - NEED to check for Stroke, then determine features to drop\n",
    "\n",
    "Data=Data.drop(labels=['RMSXacc','RMSYacc','RMSZacc'],axis=1)    #equivalent to variance if mean 0\n",
    "Data=Data.drop(labels=['meanXacc','meanYacc','meanZacc'],axis=1) #if signal is mean 0 this feature is useless\n",
    "Data=Data.drop(labels=['varXacc','varYacc','varZacc'],axis=1) #range is strongly correlated with variance\n",
    "Data=Data.drop(labels=['RMSXgyr','RMSYgyr','RMSZgyr'],axis=1)    #equivalent to variance if mean 0\n",
    "Data=Data.drop(labels=['meanXgyr','meanYgyr','meanZgyr'],axis=1) #if signal is mean 0 this feature is useless\n",
    "Data=Data.drop(labels=['varXgyr','varYgyr','varZgyr'],axis=1) #range is strongly correlated with variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understand clip data functions and see if helps resampling\n",
    "- Each subject, append all trials for each activity\n",
    "- Validation: LOSOCV - leave 1 subject out, take avg sensitivity and specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip data for accelerometer and gyro data\n",
    "- from PreprocessFcns.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***** Working on this*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract clips for accelerometer and gyro data (allows selecting start and end fraction)\n",
    "#len_tol is the % of the intended clipsize below which clip is not used\n",
    "def gen_clips(act_dict,task,location,clipsize=10000,overlap=0.9,verbose=False,startTS=0,endTS=1,\n",
    "              len_tol=1.0,resample=False):\n",
    "    \"\"\"\n",
    "    Default arg changes:\n",
    "    - changed clipsize=5000 to 10000 for 10 sec clips\n",
    "    - changed overlap=0 to 0.9 for 90% overlap b/n clips\n",
    "    - changed len_tol=0.8 to 1.0, discard clips less than 10 sec\n",
    "\n",
    "    Understand the following args:\n",
    "    startTS\n",
    "    endTS\n",
    "    \"\"\"\n",
    "    \n",
    "    clip_data = {} #the dictionary with clips\n",
    "\n",
    "    for trial in act_dict[task].keys():\n",
    "        clip_data[trial] = {}\n",
    "\n",
    "        for s in ['accel','gyro']:\n",
    "\n",
    "            if verbose:\n",
    "                print(task,' sensortype = %s - trial %d'%(s,trial))\n",
    "            #create clips and store in a list\n",
    "            rawdata = act_dict[task][trial][location][s]\n",
    "            if rawdata.empty is True: #skip if no data for current sensor\n",
    "                continue\n",
    "            #reindex time (relative to start)\n",
    "            idx = rawdata.index\n",
    "            idx = idx-idx[0]\n",
    "            rawdata.index = idx\n",
    "            #choose to create clips only on a fraction of the data (0<[startTS,endTS]<1)\n",
    "            if (startTS > 0) | (endTS < 1):\n",
    "                rawdata = rawdata.iloc[round(startTS*len(rawdata)):round(endTS*len(rawdata)),:]\n",
    "                #reindex time (relative to start)\n",
    "                idx = rawdata.index\n",
    "                idx = idx-idx[0]\n",
    "                rawdata.index = idx\n",
    "            #create clips data\n",
    "            deltat = np.median(np.diff(rawdata.index))\n",
    "            clips = []\n",
    "            #use entire recording\n",
    "            if clipsize == 0:\n",
    "                clips.append(rawdata)\n",
    "            #take clips\n",
    "            else:\n",
    "                idx = np.arange(0,rawdata.index[-1],clipsize*(1-overlap))\n",
    "                for i in idx:\n",
    "                    c = rawdata[(rawdata.index>=i) & (rawdata.index<i+clipsize)]\n",
    "                    #keep/append clips that are 10 sec, else discard those that don't meet length\n",
    "                    #tolerance\n",
    "                    ## changed > to >= so it will keep clip>=10 sec\n",
    "                    if len(c) >= len_tol*int(clipsize/deltat):\n",
    "                        clips.append(c)\n",
    "\n",
    "            #store clip length\n",
    "            #store the length of each clip\n",
    "            clip_len = [clips[c].index[-1]-clips[c].index[0] for c in range(len(clips))] \n",
    "            #assemble in dict\n",
    "            clip_data[trial][s] = {'data':clips, 'clip_len':clip_len}\n",
    "\n",
    "    return clip_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#store clips from all locations in a dataframe, indexed by the visit - NEED TO REFINE, do NOT USE\n",
    "def gen_clips_alllocs(act_dict,task,clipsize=5000,overlap=0,verbose=False,len_tol=0.95):\n",
    "\n",
    "    clip_data = pd.DataFrame()\n",
    "\n",
    "    for visit in act_dict[task].keys():\n",
    "\n",
    "        clip_data_visit=pd.DataFrame()\n",
    "\n",
    "        #loop through locations\n",
    "        for location in act_dict[task][visit].keys():\n",
    "\n",
    "            for s in ['accel','gyro']:\n",
    "\n",
    "                if verbose:\n",
    "                    print(task,' sensortype = %s - visit %d'%(s,visit))\n",
    "                #create clips and store in a list\n",
    "                rawdata = act_dict[task][visit][location][s]\n",
    "                if rawdata.empty is True: #skip if no data for current sensor\n",
    "                    continue\n",
    "                #reindex time (relative to start)\n",
    "                idx = rawdata.index\n",
    "                idx = idx-idx[0]\n",
    "                rawdata.index = idx\n",
    "                #create clips data\n",
    "                deltat = np.median(np.diff(rawdata.index))\n",
    "                clips = pd.DataFrame()\n",
    "                #take clips\n",
    "                idx = np.arange(0,rawdata.index[-1],clipsize*(1-overlap))\n",
    "                for i in idx:\n",
    "                    c = rawdata[(rawdata.index>=i) & (rawdata.index<i+clipsize)]\n",
    "                    if len(c) > len_tol*int(clipsize/deltat): #discard clips whose length is less than len_tol% of the window size\n",
    "                        df = pd.DataFrame({location+'_'+s:[c.values]},index=[visit])\n",
    "                        clips=pd.concat((clips,df))\n",
    "\n",
    "                clip_data_visit=pd.concat((clip_data_visit,clips),axis=1) #all clips from all locs for current visit\n",
    "\n",
    "        clip_data = pd.concat((clip_data_visit,clip_data)) #contains clips from all visits (index) and sensors (cols)\n",
    "\n",
    "    return clip_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refer to PreprocessFcns.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper fcns for Data Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import pickle #to save files\n",
    "from itertools import product\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import butter, welch, filtfilt\n",
    "#import nolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_spectra_welch(rawdata,fm,fM):\n",
    "    \"\"\"Power Spectral Density (PSD) on magnitude using Welch method\"\"\"\n",
    "    #compute PSD on signal magnitude\n",
    "    x = rawdata.iloc[:,-1]\n",
    "    n = len(x) #number of samples in clip\n",
    "    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "    f,Pxx_den = welch(x,Fs,nperseg=min(256,n))\n",
    "    #return PSD in desired interval of freq\n",
    "    inds = (f<=fM)&(f>=fm)\n",
    "    f=f[inds]\n",
    "    Pxx_den=Pxx_den[inds]\n",
    "    Pxxdf = pd.DataFrame(data=Pxx_den,index=f,columns=['PSD_magnitude'])\n",
    "\n",
    "    return Pxxdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(clip_data):\n",
    "    \"\"\"\n",
    "    Extract features from both sensors (accel and gyro) for current clips and trials\n",
    "    Input: dictionary of clips from each subject\n",
    "    Output: feature matrix from all clips from given subject and scores for each clip\n",
    "    \"\"\"\n",
    "    \n",
    "    features_list = ['RMSX','RMSY','RMSZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ','xcor_peakXY','xcorr_peakXZ','xcorr_peakYZ',\n",
    "                    'xcorr_lagXY','xcorr_lagXZ','xcorr_lagYZ','Dom_freq','Pdom_rel','PSD_mean','PSD_std','PSD_skew',\n",
    "                    'PSD_kur','jerk_mean','jerk_std','jerk_skew','jerk_kur','Sen_X','Sen_Y','Sen_Z']\n",
    "\n",
    "    for trial in clip_data.keys():\n",
    "\n",
    "        for sensor in clip_data[trial].keys():\n",
    "\n",
    "            #cycle through all clips for current trial and save dataframe of features for current trial and sensor\n",
    "            features = []\n",
    "            for c in range(len(clip_data[trial][sensor]['data'])):\n",
    "                rawdata = clip_data[trial][sensor]['data'][c]\n",
    "                #acceleration magnitude\n",
    "                rawdata_wmag = rawdata.copy()\n",
    "                rawdata_wmag['Accel_Mag']=np.sqrt((rawdata**2).sum(axis=1))\n",
    "\n",
    "                #extract features on current clip\n",
    "\n",
    "                #Root mean square of signal on each axis\n",
    "                N = len(rawdata)\n",
    "                RMS = 1/N*np.sqrt(np.asarray(np.sum(rawdata**2,axis=0)))\n",
    "\n",
    "                #range on each axis\n",
    "                min_xyz = np.min(rawdata,axis=0)\n",
    "                max_xyz = np.max(rawdata,axis=0)\n",
    "                r = np.asarray(max_xyz-min_xyz)\n",
    "\n",
    "                #Moments on each axis\n",
    "                mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "                var = np.asarray(np.std(rawdata,axis=0))\n",
    "                sk = skew(rawdata)\n",
    "                kurt = kurtosis(rawdata)\n",
    "\n",
    "                #Cross-correlation between axes pairs\n",
    "                xcorr_xy = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,1],mode='same')\n",
    "                # xcorr_xy = xcorr_xy/np.abs(np.sum(xcorr_xy)) #normalize values\n",
    "                xcorr_peak_xy = np.max(xcorr_xy)\n",
    "                xcorr_lag_xy = (np.argmax(xcorr_xy))/len(xcorr_xy) #normalized lag\n",
    "\n",
    "                xcorr_xz = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_xz = xcorr_xz/np.abs(np.sum(xcorr_xz)) #normalize values\n",
    "                xcorr_peak_xz = np.max(xcorr_xz)\n",
    "                xcorr_lag_xz = (np.argmax(xcorr_xz))/len(xcorr_xz)\n",
    "\n",
    "                xcorr_yz = np.correlate(rawdata.iloc[:,1],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_yz = xcorr_yz/np.abs(np.sum(xcorr_yz)) #normalize values\n",
    "                xcorr_peak_yz = np.max(xcorr_yz)\n",
    "                xcorr_lag_yz = (np.argmax(xcorr_yz))/len(xcorr_yz)\n",
    "\n",
    "                #pack xcorr features\n",
    "                xcorr_peak = np.array([xcorr_peak_xy,xcorr_peak_xz,xcorr_peak_yz])\n",
    "                xcorr_lag = np.array([xcorr_lag_xy,xcorr_lag_xz,xcorr_lag_yz])\n",
    "\n",
    "                #Dominant freq and relative magnitude (on acc magnitude)\n",
    "                Pxx = power_spectra_welch(rawdata_wmag,fm=0,fM=10)\n",
    "                domfreq = np.asarray([Pxx.iloc[:,-1].argmax()])\n",
    "                Pdom_rel = Pxx.loc[domfreq].iloc[:,-1].values/Pxx.iloc[:,-1].sum() #power at dominant freq rel to total\n",
    "\n",
    "                #moments of PSD\n",
    "                Pxx_moments = np.array([np.nanmean(Pxx.values),np.nanstd(Pxx.values),skew(Pxx.values),kurtosis(Pxx.values)])\n",
    "\n",
    "                #moments of jerk magnitude\n",
    "                jerk = rawdata.iloc[:,-1].diff().values\n",
    "                jerk_moments = np.array([np.nanmean(jerk),np.nanstd(jerk),skew(jerk[~np.isnan(jerk)]),kurtosis(jerk[~np.isnan(jerk)])])\n",
    "\n",
    "                # EMG data\n",
    "                #sample entropy raw data (magnitude) and FFT\n",
    "                sH_raw = []; sH_fft = []\n",
    "\n",
    "                for a in range(3):\n",
    "                    x = rawdata.iloc[:,a]\n",
    "                    n = len(x) #number of samples in clip\n",
    "                    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "                    sH_raw.append(nolds.sampen(x)) #samp entr raw data\n",
    "                    #for now disable SH on fft\n",
    "                    # f,Pxx_den = welch(x,Fs,nperseg=min(256,n/4))\n",
    "                    # sH_fft.append(nolds.sampen(Pxx_den)) #samp entr fft\n",
    "\n",
    "                #Assemble features in array\n",
    "                X = np.concatenate((RMS,r,mean,var,sk,kurt,xcorr_peak,xcorr_lag,domfreq,Pdom_rel,Pxx_moments,jerk_moments,sH_raw))\n",
    "                features.append(X)\n",
    "\n",
    "            F = np.asarray(features) #feature matrix for all clips from current trial\n",
    "            clip_data[trial][sensor]['features'] = pd.DataFrame(data=F,columns=features_list,dtype='float32')\n",
    "\n",
    "#     return clip_data #not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HPfilter(act_dict,task,loc,cutoff=0.75,ftype='highpass'):\n",
    "    \"\"\"\n",
    "    Highpass (or lowpass) filter data. HP to remove gravity (offset - limb orientation) from accelerometer \n",
    "    data from each visit (trial)\n",
    "    \n",
    "    Input: Activity dictionary, cutoff freq [Hz], task, sensor location and type of filter \n",
    "    (highpass or lowpass).\n",
    "    \"\"\"\n",
    "    sensor = 'accel'\n",
    "    for trial in act_dict[task].keys():\n",
    "        rawdata = act_dict[task][trial][loc][sensor]\n",
    "        if rawdata.empty is True: #skip if no data for current sensor\n",
    "            continue\n",
    "        idx = rawdata.index\n",
    "        idx = idx-idx[0]\n",
    "        rawdata.index = idx\n",
    "        x = rawdata.values\n",
    "        Fs = np.mean(1/(np.diff(rawdata.index)/1000)) #sampling rate\n",
    "        #filter design\n",
    "        cutoff_norm = cutoff/(0.5*Fs)\n",
    "        b,a = butter(4,cutoff_norm,btype=ftype,analog=False)\n",
    "        #filter data\n",
    "        xfilt = filtfilt(b,a,x,axis=0)\n",
    "        rawdatafilt = pd.DataFrame(data=xfilt,index=rawdata.index,columns=rawdata.columns)\n",
    "        act_dict[task][trial][loc][sensor] = rawdatafilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bandpass filter data (analysis of Tremor)\n",
    "#input: Activity dictionary, min,max freq [Hz], task and sensor location to filter\n",
    "def BPfilter(act_dict,task,loc,cutoff_low=3,cutoff_high=8,order=4):\n",
    "\n",
    "    sensor = 'accel'\n",
    "    for trial in act_dict[task].keys():\n",
    "        rawdata = act_dict[task][trial][loc][sensor]\n",
    "        idx = rawdata.index\n",
    "        idx = idx-idx[0]\n",
    "        rawdata.index = idx\n",
    "        x = rawdata.values\n",
    "        Fs = np.mean(1/(np.diff(rawdata.index)/1000)) #sampling rate\n",
    "        #filter design\n",
    "        cutoff_low_norm = cutoff_low/(0.5*Fs)\n",
    "        cutoff_high_norm = cutoff_high/(0.5*Fs)\n",
    "        b,a = butter(order,[cutoff_low_norm,cutoff_high_norm],btype='bandpass',analog=False)\n",
    "        #filter data\n",
    "        xfilt = filtfilt(b,a,x,axis=0)\n",
    "        rawdatafilt = pd.DataFrame(data=xfilt,index=rawdata.index,columns=rawdata.columns)\n",
    "        act_dict[task][trial][loc][sensor] = rawdatafilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filters data in all recordings (visits)\n",
    "def filterdata(act_dict,task,loc,trial,sensor='accel',ftype='highpass',cutoff=0.5,cutoff_bp=[3,8],order=4):\n",
    "\n",
    "    rawdata = act_dict[task][trial][loc][sensor]\n",
    "    if not rawdata.empty:\n",
    "        idx = rawdata.index\n",
    "        idx = idx-idx[0]\n",
    "        rawdata.index = idx\n",
    "        x = rawdata.values\n",
    "        Fs = np.mean(1/(np.diff(rawdata.index)/1000)) #sampling rate\n",
    "        if ftype != 'bandpass':\n",
    "            #filter design\n",
    "            cutoff_norm = cutoff/(0.5*Fs)\n",
    "            b,a = butter(4,cutoff_norm,btype=ftype,analog=False)\n",
    "        else:\n",
    "            #filter design\n",
    "            cutoff_low_norm = cutoff_bp[0]/(0.5*Fs)\n",
    "            cutoff_high_norm = cutoff_bp[1]/(0.5*Fs)\n",
    "            b,a = butter(order,[cutoff_low_norm,cutoff_high_norm],btype='bandpass',analog=False)\n",
    "\n",
    "        #filter data\n",
    "        xfilt = filtfilt(b,a,x,axis=0)\n",
    "        rawdatafilt = pd.DataFrame(data=xfilt,index=rawdata.index,columns=rawdata.columns)\n",
    "        act_dict[task][trial][loc][sensor] = rawdatafilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Test on subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Pickle file dict\n",
    "subj = 'HC02'\n",
    "dict_path = r'//FS2.smpp.local\\RTO\\Inpatient Sensors -Stroke\\Data\\biostamp_data\\Data_dict'\n",
    "f = open(os.path.join(dict_path, subj + 'dict.pkl'), 'rb') # use for C: directory\n",
    "act_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "act_dict = \n",
    "task = \n",
    "loc = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: clip data\n",
    "clip_data = gen_clips(act_dict,task,loc,clipsize=10000,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: extract features\n",
    "feature_extraction(clip_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract full recordings\n",
    "\n",
    "#clipsize=0 extracts full recordings\n",
    "clip_data = gen_clips(act_dict,task,loc,clipsize=0,verbose=True)\n",
    "features = feature_extraction(clip_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_data[0]['accel']['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clips\n",
    "##########################\n",
    "##########################\n",
    "clip_data = gen_clips(act_dict,task,loc,verbose=True,len_tol=0.95)\n",
    "feature_extraction(clip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clips\n",
    "clip_data = gen_clips(act_dict,task=task,clipsize=5000,location=loc,overlap=0,verbose=True,\n",
    "                      startTS=0,endTS=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sensor\n",
    "sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "clip_data[1]['gyro']['data'][0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction\n",
    "\n",
    "F = feature_extraction(clip_data)\n",
    "F.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_data[0]['accel']['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data from DataPreprocessor2_wTime.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSO Cross validation (leave one subject out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *****Refer to Models-Features-Andrew.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOSOCV(Data,X,y,groups,models,LOin=0):\n",
    "\n",
    "    subj = LeaveOneGroupOut() \n",
    "    results = pd.DataFrame(data=None,columns=['model','f1','auprc','auroc'])\n",
    "    groupres = {} #results on each group for each model\n",
    "\n",
    "\n",
    "    #train multiple classifiers\n",
    "    for m in models:\n",
    "        acc_all=[]; acc_train=[] \n",
    "        f1_test_all=[]; prec_all=[]; rec_all=[]; spec_all=[]; auprc_all=[]; auroc_train_all=[]; \n",
    "        auroc_all=[]; fpr_all=[]; tpr_all=[]; fi_all=[]\n",
    "\n",
    "        clf = m[0]; model_name = m[1]            \n",
    "        print('Training %s'%model_name)\n",
    "        s = 0\n",
    "        #LOSO CV for current classifier\n",
    "        for train_index, test_index in subj.split(X, y, groups):\n",
    "        #leave one in:\n",
    "            if LOin:\n",
    "                tridx = train_index.copy()\n",
    "                train_index = test_index.copy()\n",
    "                test_index = tridx\n",
    "            Xtr, Xte = X[train_index], X[test_index]\n",
    "            ytr, yte = y[train_index], y[test_index]\n",
    "            if len(np.unique(ytr))<2: #skip if no positive or negative examples are available for training\n",
    "                print('only 1 class available in train data - skipping')\n",
    "                continue\n",
    "            clf.fit(Xtr,ytr)\n",
    "            ypred = clf.predict(Xte)\n",
    "            yscore = clf.predict_proba(Xte)\n",
    "            yscore = yscore[:,1]\n",
    "            \n",
    "            #accuracy on train set\n",
    "            ypred_train = clf.predict(Xtr)\n",
    "            acc_train.append(sum(ypred_train==ytr)/len(ytr))\n",
    "            auroc_train = roc_auc_score(ytr,clf.predict_proba(Xtr)[:,1])\n",
    "            \n",
    "            #f1-score, prec, recall, specificity, auprc, auroc\n",
    "            f1_test_all.append(f1_score(yte,ypred))\n",
    "            precision, recall, _ = precision_recall_curve(yte,yscore)\n",
    "            auprc = auc(recall,precision)\n",
    "            if len(np.unique(yte))>1:\n",
    "                auroc = roc_auc_score(yte,yscore)\n",
    "            else:\n",
    "                print('only 1 class in test data - cannot compute roc curve')\n",
    "                auroc = np.nan\n",
    "            spec = sum((ypred==0) & (yte==0))/sum(yte==0)\n",
    "\n",
    "\n",
    "            prec_all.append(precision_score(yte,ypred))\n",
    "            rec_all.append(recall_score(yte,ypred))\n",
    "            auprc_all.append(auprc)\n",
    "            auroc_all.append(auroc)\n",
    "            auroc_train_all.append(auroc_train)\n",
    "            spec_all.append(spec)\n",
    "            \n",
    "            #compute ROC points at fixed fpr (to plot error bars)\n",
    "            fpr=np.linspace(0,1,101); tpr=[]\n",
    "\n",
    "            if len(np.unique(yte))>1:                \n",
    "                nscores = np.sort(np.column_stack((yscore[yte==0],yte[yte==0])),axis=0)\n",
    "                neg_counts = sum(yte==0)\n",
    "                for f in fpr:\n",
    "                    ind = neg_counts-int(neg_counts*f)-1\n",
    "                    t = (nscores[ind])[0]\n",
    "                    if f==1:\n",
    "                        t = 0\n",
    "                    tpr_t = sum(yscore[yte==1]>t) / sum(yte==1)\n",
    "                    tpr.append(tpr_t) \n",
    "\n",
    "            fpr = np.asarray(fpr); tpr = np.asarray(tpr)\n",
    "            fpr_all.append(fpr); tpr_all.append(tpr)\n",
    "            \n",
    "            #store feature importance\n",
    "            if model_name != 'SVM':\n",
    "                fi_all.append(clf.feature_importances_)\n",
    "            \n",
    "            print('\\nSubj/Visit %d, prec=%.3f, rec=%.3f, Spec=%.3f, auroc_train=%.3f, auroc=%.3f'%(s,precision_score(yte,ypred),recall_score(yte,ypred),\n",
    "                                                                                 spec,auroc_train,auroc))\n",
    "            s+=1\n",
    "\n",
    "        print('f1_test=%.3f+/-%.3f, prec=%.3f+/-%.3f, rec=%.3f+/-%.3f, auprc=%.3f+/-%.3f'%(\n",
    "        np.nanmean(f1_test_all),np.nanstd(f1_test_all),\n",
    "        np.nanmean(prec_all),np.nanstd(prec_all), np.nanmean(rec_all),np.nanstd(rec_all),\n",
    "        np.nanmean(auprc_all),np.nanstd(auprc_all)))\n",
    "        \n",
    "        #group results for each model\n",
    "        groupres[model_name] = {'f1':f1_test_all, 'auprc':auprc_all, 'auroc':auroc_all, 'tpr':tpr_all, 'fpr':fpr_all, \n",
    "                                'rec':rec_all, 'spec':spec_all, 'fi':fi_all}\n",
    "        \n",
    "        #mean across groups for each model\n",
    "        r = pd.DataFrame({'model':model_name, 'f1':np.nanmean(f1_test_all), 'auprc':np.nanmean(auprc_all), 'auroc':np.nanmean(auroc_all)}\n",
    "                        ,index=[0])\n",
    "        results = pd.concat((results,r))\n",
    "        \n",
    "    return results,groupres \n",
    "    \n",
    "\n",
    "#train multiple classifiers using stratified kfold\n",
    "def trainmodel_Kfold(Xf,y,models,nfolds=5,balance=False):\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    f1_fold_clf=[]; auc_fold_clf=[]; cmats={}\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=nfolds,shuffle=True,random_state=46)\n",
    "        \n",
    "    for clf,model_name in models:\n",
    "        f1_fold = []; f1_train_fold=[]; prec_fold=[]; rec_fold=[]; auprc_fold=[]\n",
    "\n",
    "        for train_idx,test_idx in skf.split(Xf,y):\n",
    "            Xtr = Xf[train_idx]\n",
    "            ytr = y[train_idx]\n",
    "            if balance:\n",
    "                #balance dataset\n",
    "                indsp = np.where(ytr==1)[0]\n",
    "                indsn = np.random.choice(np.where(ytr==0)[0],size=round(1*len(indsp)),replace=False)\n",
    "                inds = np.hstack((indsp,indsn))\n",
    "                Xtr = Xtr[inds]; ytr = ytr[inds]\n",
    "#             print(len(ytr[ytr==0])/len(ytr[ytr>0]))\n",
    "            Xte = Xf[test_idx]\n",
    "            yte = y[test_idx]\n",
    "            Xtr = scaler.fit_transform(Xtr)\n",
    "            Xte = scaler.transform(Xte)\n",
    "            clf.fit(Xtr,ytr)\n",
    "            ypred_train = clf.predict(Xtr)\n",
    "            ypred = clf.predict(Xte)\n",
    "            yscore = clf.predict_proba(Xte)\n",
    "            yscore = yscore[:,1]\n",
    "            \n",
    "            #accuracy on train and test set for current fold\n",
    "            precision, recall, _ = precision_recall_curve(yte,yscore)\n",
    "            \n",
    "            f1_train_fold.append(f1_score(ytr,ypred_train))\n",
    "            f1_fold.append(f1_score(yte,ypred))\n",
    "            prec_fold.append(precision_score(yte,ypred))\n",
    "            rec_fold.append(recall_score(yte,ypred))\n",
    "            auprc_fold.append(auc(recall,precision)\n",
    ")\n",
    "\n",
    "\n",
    "        #mean accuracy across folds\n",
    "        if balance:\n",
    "            print('Npos = %d, Nneg = %d'%(len(indsp),len(indsn)))\n",
    "            \n",
    "        print('%s'%model_name)\n",
    "        print('f1_train=%.3f+/-%.3f, f1_test=%.3f+/-%.3f, prec=%.3f+/-%.3f, rec=%.3f+/-%.3f, auprc=%.3f+/-%.3f'%(\n",
    "        np.nanmean(f1_train_fold),np.nanstd(f1_train_fold),\n",
    "        np.nanmean(f1_fold),np.nanstd(f1_fold),\n",
    "        np.nanmean(prec_fold),np.nanstd(prec_fold), np.nanmean(rec_fold),np.nanstd(rec_fold),\n",
    "        np.nanmean(auprc_fold),np.nanstd(auprc_fold)))\n",
    "        \n",
    "        cf_matrix = confusion_matrix(yte, ypred)\n",
    "        print(cf_matrix)\n",
    "        #store f1 and auc for each fold and clf\n",
    "        f1_fold_clf.append(f1_fold)\n",
    "        auc_fold_clf.append(auprc_fold)\n",
    "        #store cmat for each clf (on last fold)\n",
    "        cmats.update({model_name:cf_matrix})\n",
    "        \n",
    "    return f1_fold_clf, auc_fold_clf, cmats\n",
    "\n",
    "    \n",
    "    \n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#     print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    \n",
    "\n",
    "def plot_roc(tpr_all,fpr,roc_auc,ax=None,plotname=None,col=None):\n",
    "    #plot mean ROC across subjects (need to add shaded conf interval)\n",
    "    tprmu = np.mean(np.asarray(tpr_all),axis=0)\n",
    "    tpr=np.asarray(tpr_all)\n",
    "    fpr=np.reshape(fpr,(1,-1))\n",
    "    tprmu=np.reshape(tprmu,(1,-1))\n",
    "    label=pd.Series(data = ['%s - AUC = %0.3f' % (plotname,roc_auc)]*len(fpr))\n",
    "    if plotname=='Threshold':\n",
    "        ls = '-'\n",
    "    else:\n",
    "        ls='-'\n",
    "    if ax == None:\n",
    "        ax = sns.tsplot(data=tpr,time=fpr,ci=95,condition=label,legend=True,color=col,lw=3,linestyle=ls)\n",
    "    else:\n",
    "        sns.tsplot(data=tpr,time=fpr,ci=95,condition=label, legend=True,ax=ax,color=col,lw=3,linestyle=ls)\n",
    "             \n",
    "    lw = 3\n",
    "    ax.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    ax.set_xlim([-0.05, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate',fontsize=16)\n",
    "    ax.set_ylabel('True Positive Rate',fontsize=16)\n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "Data = pd.read_hdf(os.path.join(features_path,'Features_AllLocsHP+LP.hdf5'))\n",
    "print(Data.shape)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter empty rows (no features available)\n",
    "print('discarded %d rows'%(len(Data[Data.dorsal_hand__accel.apply(type) == float])))\n",
    "Data = Data[Data.dorsal_hand__accel.apply(type) != float]\n",
    "print('discarded %d rows'%(len(Data[Data.dorsal_hand__gyro.apply(type) == float])))\n",
    "Data = Data[Data.dorsal_hand__gyro.apply(type) != float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.Task.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = list(Data.iloc[:,7:].columns)\n",
    "sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack features\n",
    "colnames=['RMSX', 'RMSY', 'RMSZ', 'rangeX', 'rangeY', 'rangeZ', 'meanX',\n",
    "       'meanY', 'meanZ', 'varX', 'varY', 'varZ', 'skewX', 'skewY', 'skewZ',\n",
    "       'kurtX', 'kurtY', 'kurtZ', 'xcor_peakXY', 'xcorr_peakXZ',\n",
    "       'xcorr_peakYZ', 'xcorr_lagXY', 'xcorr_lagXZ', 'xcorr_lagYZ', 'Dom_freq',\n",
    "       'Pdom_rel', 'PSD_mean', 'PSD_std', 'PSD_skew', 'PSD_kur', 'jerk_mean',\n",
    "       'jerk_std', 'jerk_skew', 'jerk_kur', 'Sen_X', 'Sen_Y', 'Sen_Z']\n",
    "\n",
    "colnames1=[i+'acc' for i in colnames]\n",
    "colnames2=[i+'gyr' for i in colnames]\n",
    "colnames=colnames1+colnames2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datanew = pd.DataFrame(columns=colnames)\n",
    "for i in range(len(Data)):\n",
    "    F1 = Data.dorsal_hand__accel.iloc[i]\n",
    "    F2 = Data.dorsal_hand__gyro.iloc[i]\n",
    "    F = pd.DataFrame(data=np.hstack((F1,F2)).reshape(-1,1).T,index=[i],columns=colnames)\n",
    "    Datanew=pd.concat((Datanew,F),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datanew=Datanew.reset_index(drop=True)\n",
    "Data=Data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pd.concat((Data.iloc[:,:7],Datanew),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saving unpacked features...')\n",
    "Data.to_hdf(os.path.join(features_path,'UnpackedFeatures_Hands_HP+LP.hdf5'),'w')\n",
    "print('unpacked features saved in %s'%(features_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
