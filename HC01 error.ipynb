{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug HC01 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd Y:\\Inpatient Sensors -Stroke\\Data\\biostamp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import os\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import pickle #to save files\n",
    "from itertools import product\n",
    "from scipy.stats import skew, kurtosis, pearsonr\n",
    "from scipy.signal import butter, welch, filtfilt, resample\n",
    "import time\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Windows':\n",
    "    if platform.release() == '7':\n",
    "        path = r'Y:\\Inpatient Sensors -Stroke\\Data\\biostamp_data\\controls'\n",
    "        folder_path = r'Y:\\Inpatient Sensors -Stroke\\Data\\biostamp_data'\n",
    "        dict_path = r'Y:\\Inpatient Sensors -Stroke\\Data\\biostamp_data\\Data_dict'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List based on Value data of Activity Recognition\n",
    "complete= list(['LYING','SITTING','STANDING','WALKING','STAIRS DOWN','STAIRS UP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data without 'trial' structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_annotations(path):\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# Processes raw annotations file to extract start / end timestamps and remove unnecessary data\n",
    "#\n",
    "# Inputs:  path - filepath of the subject folder containing annotations.csv\n",
    "#\n",
    "# Outputs: df - dataframe containing list of activities and their start / end timestamps\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "    df = pd.read_csv(os.path.join(path, 'annotations.csv'))\n",
    "    del df['Timestamp (ms)']\n",
    "    del df['AnnotationId']\n",
    "    del df['AuthorId']\n",
    "    \n",
    "    # subset Activity Recognition data by partially match EventType string\n",
    "    df = df[df['EventType'].str.match('Activity')]\n",
    "    del df['EventType']\n",
    "    df.Value = df.Value.shift(-1)\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Create Trial column for Value\n",
    "    sorter = set(df.Value.unique().flatten())\n",
    "    sorterIndex = dict(zip(sorter, range(len(sorter))))        \n",
    "    df['Value_Rank'] = df['Value'].map(sorterIndex)\n",
    "    df['Trial'] = df.groupby('Value')['Start Timestamp (ms)'].rank(ascending=True).astype(int)\n",
    "    del df['Value_Rank']\n",
    "    df = df.reset_index(drop=True).set_index('Value')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying without Trial\n",
    "\n",
    "#For a given subject, extracts and separates accelerometer, gyroscope, and \n",
    "#EMG/ECG data into trials and sensor per activity\n",
    "def  extract_data(SubID, path):\n",
    "\n",
    "    ## This is the annotations.csv dataset cleaned\n",
    "    ## Used to match timestamp ranges to the accel, gyro, elec data\n",
    "    timestamps = process_annotations(path)\n",
    "#    timestamps = fix_errors(SubID, timestamps)\n",
    "#    timestamps = add_unstruct_data(timestamps)\n",
    "    \n",
    "    # Creates list of sensor locations from folders within subject's raw data directory\n",
    "    locations = [locs for locs in os.listdir(path) if os.path.isdir(os.path.join(path, locs))]\n",
    "    \n",
    "    # Creates dictionary of empty dataframes to merge all accelerometer, gyroscope, and EMG/ECG data for each sensor\n",
    "    accel = {locs: pd.DataFrame() for locs in locations}\n",
    "    gyro = {locs: pd.DataFrame() for locs in locations}\n",
    "    elec = {locs: pd.DataFrame() for locs in locations}\n",
    "    \n",
    "    # Finds and merges all accelerometer, gyroscope, and EMG/ECG data for each sensor, retains datetime information\n",
    "    for root, dirs, files in os.walk(path, topdown=True):\n",
    "        for filenames in files:\n",
    "            if filenames.endswith('accel.csv'):\n",
    "                p = pathlib.Path(os.path.join(root, filenames))\n",
    "                location = str(p.relative_to(path)).split(\"\\\\\")[0]\n",
    "                temp_df = pd.read_csv(p).set_index('Timestamp (ms)')\n",
    "                accel[location] = accel[location].append(temp_df)\n",
    "\n",
    "            elif filenames.endswith('gyro.csv'):\n",
    "                p = pathlib.Path(os.path.join(root, filenames))\n",
    "                location = str(p.relative_to(path)).split(\"\\\\\")[0]\n",
    "                temp_df = pd.read_csv(p).set_index('Timestamp (ms)')\n",
    "                gyro[location] = gyro[location].append(temp_df)\n",
    "\n",
    "            elif filenames.endswith('elec.csv'):\n",
    "                p = pathlib.Path(os.path.join(root, filenames))\n",
    "                location = str(p.relative_to(path)).split(\"\\\\\")[0]\n",
    "                temp_df = pd.read_csv(p).set_index('Timestamp (ms)')\n",
    "                elec[location] = elec[location].append(temp_df)\n",
    "                \n",
    "    complete_acts = complete\n",
    "    \n",
    "    # Complete dictionary of all activities\n",
    "    act_dict = {acts: pd.DataFrame() for acts in complete_acts}\n",
    "    \n",
    "    # Populate dictionary keys per activity with every sensor\n",
    "    for activities in complete_acts:\n",
    "        \n",
    "        startSize = timestamps.loc[activities, 'Start Timestamp (ms)']\n",
    "        \n",
    "        if np.size(startSize) == 1:\n",
    "            startTimestamp = timestamps.loc[activities, 'Start Timestamp (ms)']\n",
    "            endTimestamp = timestamps.loc[activities, 'Stop Timestamp (ms)']\n",
    "        else:\n",
    "            startTimestamp = timestamps.loc[activities, 'Start Timestamp (ms)'].values\n",
    "            endTimestamp = timestamps.loc[activities, 'Stop Timestamp (ms)'].values\n",
    "\n",
    "        # Create sensor location dictionary with each key corresponding to sensor locations\n",
    "        sensor_dict = {locs: pd.DataFrame() for locs in locations}\n",
    "\n",
    "        # Extract sensor data and populate sensor_dict with sensor data\n",
    "        for location in locations:\n",
    "            print(location)#######################################\n",
    "\n",
    "            data = {'accel': pd.DataFrame(), 'gyro': pd.DataFrame(), 'elec': pd.DataFrame()}\n",
    "\n",
    "            if not accel[location].empty:\n",
    "                accelData = accel[location]\n",
    "                data['accel'] = accelData[(accelData.index >= startTime) & (accelData.index <= endTime)]  \n",
    "                ###########\n",
    "                ###########\n",
    "                #print(bool(data))\n",
    "                #if not bool(data):\n",
    "                #    data['accel'] = accelData[(accelData.index >= startTimestamp) & (accelData.index <= endTimestamp)]\n",
    "                #    print(bool(data))\n",
    "                #    print('added data') ###########\n",
    "                #else:\n",
    "                #    data['accel'] = data['accel'].append(accelData[(accelData.index >= startTimestamp) & (accelData.index <= endTimestamp)])\n",
    "\n",
    "            if not gyro[location].empty:\n",
    "                gyroData = gyro[location]\n",
    "                data['gyro'] = gyroData[(gyroData.index >= startTimestamp) & (gyroData.index <= endTimestamp)]\n",
    "\n",
    "            if not elec[location].empty:\n",
    "                elecData = elec[location]\n",
    "                data['elec'] = elecData[(elecData.index >= startTimestamp) & (elecData.index <= endTimestamp)]\n",
    "\n",
    "            sensor_dict[location] = data\n",
    "\n",
    "        act_dict[activities] = sensor_dict\n",
    "    \n",
    "    return act_dict, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubID = 'HC02'\n",
    "timestamps = process_annotations(os.path.join(path, SubID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually extract HC01 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SubID)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path2 = r'Y:\\Inpatient Sensors -Stroke\\Data\\biostamp_data\\HC01test'\n",
    "SubID = 'HC01'\n",
    "path2 = os.path.join(path, SubID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict, timestamps = extract_data(SubID, os.path.join(path, SubID))\n",
    "print('Extract data complete.')\n",
    "filename = os.path.join(dict_path2, SubID + 'dict.pkl')\n",
    "with open(filename,'wb') as f:\n",
    "    pickle.dump(act_dict,f)\n",
    "print(filename + ' ' + 'File Saved\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = process_annotations(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(dict_path2, SubID + 'dict.pkl')\n",
    "with open(filename,'wb') as f:\n",
    "    pickle.dump(act_dict,f)\n",
    "print(filename + ' ' + 'File Saved\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sacrum walking\n",
    "rawdata = act_dict['WALKING']['sacrum']['accel']\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load HC02 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Pickle file dict\n",
    "subj = 'HC02'\n",
    "f = open(os.path.join(dict_path, subj + 'dict.pkl'), 'rb') # use for C: directory\n",
    "act_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict['WALKING'][0]['sacrum']['accel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sacrum walking\n",
    "rawdata = act_dict['WALKING'][0]['sacrum']['accel']\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sacrum walking\n",
    "rawdata = act_dict['WALKING'][1]['sacrum']['accel']\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sacrum walking\n",
    "rawdata = act_dict['WALKING'][2]['sacrum']['accel']\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sacrum walking\n",
    "rawdata = act_dict['WALKING'][3]['sacrum']['accel']\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sacrum walking - all trials\n",
    "rawdata = act_dict['WALKING'][0]['sacrum']['accel']\n",
    "rawdata = rawdata.append(act_dict['WALKING'][1]['sacrum']['accel'])\n",
    "rawdata = rawdata.append(act_dict['WALKING'][2]['sacrum']['accel'])\n",
    "rawdata = rawdata.append(act_dict['WALKING'][3]['sacrum']['accel'])\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rawdata.head(5))\n",
    "print(rawdata.tail(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract function for graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** modify gen_clips function to add all trials into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract clips for accelerometer and gyro data (allows selecting start and end fraction)\n",
    "#lentol is the % of the intended clipsize below which clip is not used\n",
    "def gen_clips(act_dict,task,location,clipsize=5000,overlap=0,verbose=False,startTS=0,endTS=1,len_tol=0.8,resample=False):\n",
    "\n",
    "    clip_data = {} #the dictionary with clips\n",
    "\n",
    "    for trial in act_dict[task].keys():\n",
    "#        clip_data[trial] = {}\n",
    "\n",
    "        for s in ['accel','gyro']:\n",
    "\n",
    "            if verbose:\n",
    "                print(task,' sensortype = %s - trial %d'%(s,trial))\n",
    "            #create clips and store in a list\n",
    "            rawdata = act_dict[task][trial][location][s]\n",
    "            if rawdata.empty is True: #skip if no data for current sensor\n",
    "                continue\n",
    "            #reindex time (relative to start)\n",
    "            idx = rawdata.index\n",
    "            idx = idx-idx[0]\n",
    "            rawdata.index = idx\n",
    "            #choose to create clips only on a fraction of the data (0<[startTS,endTS]<1)\n",
    "            if (startTS > 0) | (endTS < 1):\n",
    "                rawdata = rawdata.iloc[round(startTS*len(rawdata)):round(endTS*len(rawdata)),:]\n",
    "                #reindex time (relative to start)\n",
    "                idx = rawdata.index\n",
    "                idx = idx-idx[0]\n",
    "                rawdata.index = idx\n",
    "            #create clips data\n",
    "            deltat = np.median(np.diff(rawdata.index))\n",
    "            clips = []\n",
    "            #use entire recording\n",
    "            if clipsize == 0:\n",
    "                clips.append(rawdata)\n",
    "            #take clips\n",
    "            else:\n",
    "                idx = np.arange(0,rawdata.index[-1],clipsize*(1-overlap))\n",
    "                for i in idx:\n",
    "                    c = rawdata[(rawdata.index>=i) & (rawdata.index<i+clipsize)]\n",
    "                    if len(c) > len_tol*int(clipsize/deltat): #discard clips whose length is less than len_tol% of the window size\n",
    "                        clips.append(c)\n",
    "\n",
    "            #store clip length\n",
    "            clip_len = [clips[c].index[-1]-clips[c].index[0] for c in range(len(clips))] #store the length of each clip\n",
    "            #assemble in dict\n",
    "            #clip_data[trial][s] = {'data':clips, 'clip_len':clip_len}\n",
    "            clip_data[s] = {'data':clips, 'clip_len':clip_len}\n",
    "\n",
    "    return clip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features from both sensors (accel and gyro) for current clips and trials\n",
    "#input: dictionary of clips from each subject\n",
    "#output: feature matrix from all clips from given subject and scores for each clip\n",
    "def feature_extraction(clip_data):\n",
    "\n",
    "    features_list = ['RMSX','RMSY','RMSZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ','xcor_peakXY','xcorr_peakXZ','xcorr_peakYZ',\n",
    "                    'xcorr_lagXY','xcorr_lagXZ','xcorr_lagYZ','Dom_freq','Pdom_rel','PSD_mean','PSD_std','PSD_skew',\n",
    "                    'PSD_kur','jerk_mean','jerk_std','jerk_skew','jerk_kur','Sen_X','Sen_Y','Sen_Z']\n",
    "\n",
    "    for trial in clip_data.keys():\n",
    "\n",
    "        for sensor in clip_data[trial].keys():\n",
    "\n",
    "            #cycle through all clips for current trial and save dataframe of features for current trial and sensor\n",
    "            features = []\n",
    "            for c in range(len(clip_data[trial][sensor]['data'])):\n",
    "                rawdata = clip_data[trial][sensor]['data'][c]\n",
    "                #acceleration magnitude\n",
    "                rawdata_wmag = rawdata.copy()\n",
    "                rawdata_wmag['Accel_Mag']=np.sqrt((rawdata**2).sum(axis=1))\n",
    "\n",
    "                #extract features on current clip\n",
    "\n",
    "                #Root mean square of signal on each axis\n",
    "                N = len(rawdata)\n",
    "                RMS = 1/N*np.sqrt(np.asarray(np.sum(rawdata**2,axis=0)))\n",
    "\n",
    "                #range on each axis\n",
    "                min_xyz = np.min(rawdata,axis=0)\n",
    "                max_xyz = np.max(rawdata,axis=0)\n",
    "                r = np.asarray(max_xyz-min_xyz)\n",
    "\n",
    "                #Moments on each axis\n",
    "                mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "                var = np.asarray(np.std(rawdata,axis=0))\n",
    "                sk = skew(rawdata)\n",
    "                kurt = kurtosis(rawdata)\n",
    "\n",
    "                #Cross-correlation between axes pairs\n",
    "                xcorr_xy = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,1],mode='same')\n",
    "                # xcorr_xy = xcorr_xy/np.abs(np.sum(xcorr_xy)) #normalize values\n",
    "                xcorr_peak_xy = np.max(xcorr_xy)\n",
    "                xcorr_lag_xy = (np.argmax(xcorr_xy))/len(xcorr_xy) #normalized lag\n",
    "\n",
    "                xcorr_xz = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_xz = xcorr_xz/np.abs(np.sum(xcorr_xz)) #normalize values\n",
    "                xcorr_peak_xz = np.max(xcorr_xz)\n",
    "                xcorr_lag_xz = (np.argmax(xcorr_xz))/len(xcorr_xz)\n",
    "\n",
    "                xcorr_yz = np.correlate(rawdata.iloc[:,1],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_yz = xcorr_yz/np.abs(np.sum(xcorr_yz)) #normalize values\n",
    "                xcorr_peak_yz = np.max(xcorr_yz)\n",
    "                xcorr_lag_yz = (np.argmax(xcorr_yz))/len(xcorr_yz)\n",
    "\n",
    "                #pack xcorr features\n",
    "                xcorr_peak = np.array([xcorr_peak_xy,xcorr_peak_xz,xcorr_peak_yz])\n",
    "                xcorr_lag = np.array([xcorr_lag_xy,xcorr_lag_xz,xcorr_lag_yz])\n",
    "\n",
    "                #Dominant freq and relative magnitude (on acc magnitude)\n",
    "                Pxx = power_spectra_welch(rawdata_wmag,fm=0,fM=10)\n",
    "                domfreq = np.asarray([Pxx.iloc[:,-1].argmax()])\n",
    "                Pdom_rel = Pxx.loc[domfreq].iloc[:,-1].values/Pxx.iloc[:,-1].sum() #power at dominant freq rel to total\n",
    "\n",
    "                #moments of PSD\n",
    "                Pxx_moments = np.array([np.nanmean(Pxx.values),np.nanstd(Pxx.values),skew(Pxx.values),kurtosis(Pxx.values)])\n",
    "\n",
    "                #moments of jerk magnitude\n",
    "                jerk = rawdata.iloc[:,-1].diff().values\n",
    "                jerk_moments = np.array([np.nanmean(jerk),np.nanstd(jerk),skew(jerk[~np.isnan(jerk)]),kurtosis(jerk[~np.isnan(jerk)])])\n",
    "\n",
    "                #sample entropy raw data (magnitude) and FFT\n",
    "                sH_raw = []; sH_fft = []\n",
    "\n",
    "                for a in range(3):\n",
    "                    x = rawdata.iloc[:,a]\n",
    "                    n = len(x) #number of samples in clip\n",
    "                    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "                    sH_raw.append(nolds.sampen(x)) #samp entr raw data\n",
    "                    #for now disable SH on fft\n",
    "                    # f,Pxx_den = welch(x,Fs,nperseg=min(256,n/4))\n",
    "                    # sH_fft.append(nolds.sampen(Pxx_den)) #samp entr fft\n",
    "\n",
    "                #Assemble features in array\n",
    "                X = np.concatenate((RMS,r,mean,var,sk,kurt,xcorr_peak,xcorr_lag,domfreq,Pdom_rel,Pxx_moments,jerk_moments,sH_raw))\n",
    "                features.append(X)\n",
    "\n",
    "            F = np.asarray(features) #feature matrix for all clips from current trial\n",
    "            clip_data[trial][sensor]['features'] = pd.DataFrame(data=F,columns=features_list,dtype='float32')\n",
    "\n",
    "#     return clip_data #not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore features from individual subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ...Skip HC01 until error is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Pickle file dict\n",
    "subj = 'HC02'\n",
    "f = open(os.path.join(dict_path, subj + 'dict.pkl'), 'rb')\n",
    "act_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose task, sensor location, data type, and trials\n",
    "\n",
    "#task = 'LYING'\n",
    "#task = 'SITTING'\n",
    "#task = 'STANDING'\n",
    "task = 'WALKING'\n",
    "#task = 'STAIRS DOWN'\n",
    "#task = 'STAIRS UP'\n",
    "\n",
    "#loc = 'bicep_left'\n",
    "#loc = 'bicep_right'\n",
    "#loc = 'biceps_femoris_left'\n",
    "#loc = 'biceps_femoris_right'\n",
    "#loc = 'distal_lateral_shank_left' # has accel and gyro\n",
    "#loc = 'distal_lateral_shank_right' # has accel and gyro\n",
    "#loc = 'gastrocnemius_left'\n",
    "#loc = 'gastrocnemius_right'\n",
    "#loc = 'medial_chest'\n",
    "#loc = 'posterior_forearm_left'\n",
    "#loc = 'posterior_forearm_right'\n",
    "#loc = 'rectus_femoris_left'\n",
    "#loc = 'rectus_femoris_right'\n",
    "loc = 'sacrum' # has accel and gyro\n",
    "#loc = 'tibialis_anterior_left'\n",
    "#loc = 'tibialis_anterior_right'\n",
    "\n",
    "## want a + g\n",
    "sensor = 'accel'\n",
    "#sensor = 'gyro'\n",
    "#sensor = 'elec'\n",
    "\n",
    "trial = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activity dictionary structure (TASK-TRIAL-LOCATION-SENSOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacrum walking\n",
    "rawdata = act_dict['WALKING'][0]['sacrum']['accel']\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract by sensortype and accel/gyro, but split into trials\n",
    "#clipsize!=0 extracts full recordings\n",
    "clip_data = gen_clips(act_dict,task='WALKING',location='sacrum',clipsize=0,verbose=True)\n",
    "#feature_extraction(clip_data)\n",
    "#clip_data[0]['accel']['features']\n",
    "clip_data[3]['accel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify function to create 1 trial\n",
    "def one_trial(act_dict,task,location,verbose=False)#,clipsize=5000,overlap=0,startTS=0,endTS=1,len_tol=0.8,resample=False):\n",
    "\n",
    "    clip_data = {} #the dictionary with clips\n",
    "    \n",
    "    for trial in act_dict[task].keys():\n",
    "#        clip_data[trial] = {}\n",
    "\n",
    "        for s in ['accel','gyro']:\n",
    "\n",
    "            if verbose:\n",
    "                print(task,' sensortype = %s - trial %d'%(s,trial))\n",
    "            #create clips and store in a list\n",
    "            rawdata = act_dict[task][trial][location][s]\n",
    "            if rawdata.empty is True: #skip if no data for current sensor\n",
    "                continue\n",
    "            #reindex time (relative to start)\n",
    "            idx = rawdata.index + idx\n",
    "            idx = idx-idx[0]\n",
    "            rawdata.index = idx\n",
    "            \n",
    "            #create clips data\n",
    "            #deltat = np.median(np.diff(rawdata.index))\n",
    "            clips = []\n",
    "            #use entire recording\n",
    "            #if clipsize == 0:\n",
    "            clips.append(rawdata)\n",
    "            #take clips\n",
    "            #else:\n",
    "            #    idx = np.arange(0,rawdata.index[-1],clipsize*(1-overlap))\n",
    "            #    for i in idx:\n",
    "            #        c = rawdata[(rawdata.index>=i) & (rawdata.index<i+clipsize)]\n",
    "            #        if len(c) > len_tol*int(clipsize/deltat): #discard clips whose length is less than len_tol% of the window size\n",
    "            #            clips.append(c)\n",
    "\n",
    "            #store clip length\n",
    "            clip_len = [clips[c].index[-1]-clips[c].index[0] for c in range(len(clips))] #store the length of each clip\n",
    "            #assemble in dict\n",
    "            clip_data[s] = {'data':clips, 'clip_len':clip_len}\n",
    "           \n",
    "\n",
    "#    idx2 = \n",
    "\n",
    "#    for trial in clip_data[trial]\n",
    "    ## append and add the last time stamp\n",
    "#        idx = rawdata.index # trial\n",
    "#        idx2 = idx2 + max(idx) + 8 # ms\n",
    "#        rawdata.index = idx\n",
    "    #######################\n",
    "    \n",
    "            \n",
    "    return clip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = clip_data[0]['accel']\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HCO2 walking sacrum accel - all trials\n",
    "# 4295x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = clip_data\n",
    "rawdata.plot(figsize=(8,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract clips\n",
    "clip_data = gen_clips(act_dict,task,loc,verbose=True,len_tol=0.95)\n",
    "feature_extraction(clip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
