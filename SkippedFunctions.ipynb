{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipped Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataAggregator versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edited version with omitted code chunks\n",
    "\n",
    "# Use DataAggregator_alllocs function instead? \n",
    "# https://github.com/kamshee/CIS_PD/blob/master/DataPreprocessor_AllLocs2.ipynb\n",
    "def DataAggregator(tasks,locs,clipsize=10000,overlap=0.9,startTS=0,endTS=1,append=1):#,symptom=None):\n",
    "    \"\"\"\n",
    "    Agregate features (input) and metadata(output) for classification stage\n",
    "    Apend is a flag on whether to create a new data matrix or aggregate new rows to existing one\n",
    "    startTS and endTS specificies fraction of raw sensors data to use to generate clips\n",
    "\n",
    "    OMIT: side specificies which score side to use (recommended to use the same side of the loca)\n",
    "    MODIFY: symptom specifies which filter to use. None only applies a HP filter to the data\n",
    "    \"\"\"\n",
    "    t0=time.time()\n",
    "\n",
    "    #load subjects features data and assemble with subj metadata\n",
    "    d = os.listdir(dict_path)\n",
    "    fnames = [filename[0:4] for filename in d if filename.startswith('HC')]\n",
    "\n",
    "    Data = pd.DataFrame() #the table with features and metadata\n",
    "    \n",
    "    #loop through subjects\n",
    "    print(fnames)\n",
    "    for subj_filename in fnames:\n",
    "\n",
    "        #load current subject sensor data\n",
    "        subj = int(subj_filename[2:4]) #subj numerical code\n",
    "        f = open(os.path.join(dict_path,subj_filename),'rb')    \n",
    "        act_dict = pickle.load(f)\n",
    "        f.close()\n",
    "        print('\\nLoaded Subj %s sensor data'%subj)\n",
    "\n",
    "        #loop through tasks for current subject\n",
    "        for task in tasks: \n",
    "            print(task)\n",
    "\n",
    "            #compute features from each sensor location and aggregate with metadata for current side           \n",
    "\n",
    "            #loop through sensor locations and extract data - locations match the side\n",
    "            for loc in locs:\n",
    "                if loc in list(['medial_chest','sacrum']):\n",
    "                    loc = loc\n",
    "                else:\n",
    "                    loc = loc+side                        \n",
    "                print(loc)\n",
    "\n",
    "                #high pass filter accelerometer data\n",
    "                HPfilter(act_dict,task,loc)\n",
    "\n",
    "                #generate clips and extract features for current task and sensor location\n",
    "                clip_data = gen_clips(act_dict,task,loc,clipsize,overlap,False,startTS,endTS)\n",
    "                feature_extraction(clip_data)\n",
    "                #aggregate subject and features data\n",
    "                n_rec = len(clip_data.keys()) #of sensor recordings\n",
    "                print('Number of recordings = %d, location: %s'%(n_rec,loc))\n",
    "                N = n_rec\n",
    "\n",
    "                #aggregate data from each trial for current subject\n",
    "                for i in range(N):\n",
    "                    #features\n",
    "                    # fix to avoid key error for certain sensors that ran out of battery, no data\n",
    "                    if 'accel' in clip_data[i].keys():\n",
    "                        if 'features' in clip_data[i]['accel'].keys():\n",
    "                            D = clip_data[i]['accel']['features']\n",
    "                            print('trial %d: %d clips generated'%(i,np.shape(D)[0]))\n",
    "                            featcols = D.columns.tolist()\n",
    "\n",
    "                            #metadata\n",
    "# get trial value\n",
    "                            D['Trial'] = #subj_score.visit[i]  # changed visit to trial\n",
    "                            D['Task'] = task\n",
    "                            D['Location'] = loc \n",
    "                            D['Subject'] = subj\n",
    "                            Data = pd.concat([Data,D]) #concatenate data from each visit\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "    cols = ['Subject','Trial','Task','Location']+ featcols #, 'Visit','Bradykinesia','Tremor']+ featcols\n",
    "    Data = Data[cols]    \n",
    "    print('\\nData matrix generated')\n",
    "    print(Data.shape)\n",
    "\n",
    "    saved_filename = 'DataFinal'+'.csv'#+symptom+'.csv'\n",
    "    if append and os.path.isfile(os.path.join(features_path,saved_filename)):\n",
    "        Data.to_csv(os.path.join(features_path,saved_filename),mode='a+',header=False) #append to existing\n",
    "        print('Appending to existing Feature matrix ' + str(os.path.join(features_path,saved_filename)))    \n",
    "\n",
    "    else:\n",
    "        Data.to_csv(os.path.join(features_path,saved_filename))\n",
    "        print('Feature matrix saved in ' + str(os.path.join(features_path,saved_filename)))\n",
    "    \n",
    "    t = time.time()\n",
    "    eltime = (t-t0)/60\n",
    "    print('Elapsed time = %.2f min'%eltime)\n",
    "\n",
    "    #     return Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original with ommited parts commented out\n",
    "def DataAggregator(tasks,task_scores_all,locs,clipsize=10000,overlap=0.9,startTS=0,endTS=1,append=1):#,symptom=None):\n",
    "    \"\"\"\n",
    "    Agregate features (input) and metadata(output) for classification stage\n",
    "    Apend is a flag on whether to create a new data matrix or aggregate new rows to existing one\n",
    "    startTS and endTS specificies fraction of raw sensors data to use to generate clips\n",
    "\n",
    "    OMIT: side specificies which score side to use (recommended to use the same side of the loca)\n",
    "    MODIFY: symptom specifies which filter to use. None only applies a HP filter to the data\n",
    "    \"\"\"\n",
    "    t0=time.time()\n",
    "#     #load subject scores (std motor assessments)\n",
    "#     mot_scores = pd.read_excel(os.path.join(scores_path, 'Scores.xls'))\n",
    "   \n",
    "    #load subjects features data and assemble with subj metadata\n",
    "    d = os.listdir(dict_path)\n",
    "    fnames = [filename[0:4] for filename in d if filename.startswith('HC')]\n",
    "\n",
    "    Data = pd.DataFrame() #the table with features and metadata\n",
    "    \n",
    "    #loop through subjects\n",
    "    print(fnames)\n",
    "    for subj_filename in fnames:\n",
    "\n",
    "        #load current subject sensor data\n",
    "        subj = int(subj_filename[2:4]) #subj numerical code\n",
    "        f = open(os.path.join(dict_path,subj_filename),'rb')    \n",
    "        act_dict_full = pickle.load(f) # change to act_dict since side doesn't matter\n",
    "        f.close()\n",
    "        print('\\nLoaded Subj %s sensor data'%subj)\n",
    "\n",
    "        T = tasks #make iterator\n",
    "#         T = zip(tasks,task_scores_all) #make iterator\n",
    "        \n",
    "#         act_dict_left = gen_unimodal_data(act_dict_full, side='left')\n",
    "#         act_dict_right = gen_unimodal_data(act_dict_full, side='right')\n",
    "\n",
    "        #loop through tasks for current subject\n",
    "        for task in T: \n",
    "#         for task,task_scores in T: \n",
    "            \n",
    "            print(task)\n",
    "#             ts = task_scores #copy variable\n",
    "\n",
    "            #extract scores data for left and right side for current task\n",
    "#             for side in ['left','right']:\n",
    "                \n",
    "#                 if side == 'left':\n",
    "#                     act_dict = act_dict_left\n",
    "#                 elif side == 'right':\n",
    "#                     act_dict = act_dict_right\n",
    "#                 else:\n",
    "#                     act_dict = act_dict_full\n",
    "                    \n",
    "#                 #deal with exception for alternating and finger to nose (unilateral tasks)\n",
    "#                 if (task_scores.find('alternating') > -1) | (task_scores.find('finger to nose') > -1):\n",
    "#                     task_scores=ts+ ' ' + side\n",
    "                \n",
    "#                 print('Aggregating scores for %s side'%side)\n",
    "            \n",
    "#                 #extract scores for corresponding task, subject and side\n",
    "#                 subj_score = mot_scores.loc[mot_scores['subject']==subj,['subject','visit',\n",
    "#                                             task_scores+ ' ' + 'bradykinesia ' + side + ' upper limb',\n",
    "#                                             task_scores+ ' ' + 'tremor ' + side + ' upper limb']]\n",
    "#                 #change 1 month label to 4 wks for proper sorting\n",
    "#                 mot_scores.loc[mot_scores['visit']=='1 Month','visit']='4 Weeks'\n",
    "#                 #rename cols\n",
    "#                 subj_score = subj_score.rename(index=str,\n",
    "#                                                columns={subj_score.columns[2]:'Bradykinesia',subj_score.columns[3]:'Tremor'})\n",
    "#                 print(subj_score.head())\n",
    "\n",
    "#                 subj_score.index = range(len(subj_score))\n",
    "#                 if len(subj_score) < 1:\n",
    "#                     print('no scores data for subject %d -- skipping..'%subj)\n",
    "\n",
    "                #compute features from each sensor location and aggregate with metadata for current side\n",
    "                else:             \n",
    "                    \n",
    "                    #loop through sensor locations and extract data - locations match the side\n",
    "                    for loc in locs:\n",
    "                        if loc in list(['medial_chest','sacrum']):\n",
    "                            loc = loc\n",
    "                        else:\n",
    "                            loc = loc+side                        \n",
    "                        print(loc)\n",
    "\n",
    "                        #high pass filter accelerometer data\n",
    "                        HPfilter(act_dict,task,loc)\n",
    "\n",
    "                        #generate clips and extract features for current task and sensor location\n",
    "# modify gen_clips\n",
    "                        clip_data = gen_clips(act_dict,task,loc,clipsize,overlap,False,startTS,endTS)\n",
    "                        feature_extraction(clip_data)\n",
    "                        #aggregate subject, scores and features data\n",
    "#                         n_visits = len(subj_score)    #of visits in Database\n",
    "                        n_rec = len(clip_data.keys()) #of sensor recordings\n",
    "                        print('Number of recordings = %d, location: %s'%(n_rec,loc))\n",
    "#                         print('n_visits in score file = %d, # recordings = %d, location: %s'%(n_visits,n_rec,loc))\n",
    "#                         N = n_visits\n",
    "                        N = n_rec\n",
    "\n",
    "#                         if n_visits != n_rec:\n",
    "#                             print('# of recordings does not match # of visits! - matching first %d recordings'%(min([n_visits,n_rec])))\n",
    "#                             N = min([n_visits,n_rec])\n",
    "\n",
    "                        #aggregate data from each visit for current subject\n",
    "                        for i in range(N):\n",
    "                            #features\n",
    "                            # fix to avoid key error for certain sensors that ran out of battery, no data\n",
    "                            if 'accel' in clip_data[i].keys():\n",
    "                                if 'features' in clip_data[i]['accel'].keys():\n",
    "                                    D = clip_data[i]['accel']['features']\n",
    "                                    print('trial %d: %d clips generated'%(i,np.shape(D)[0]))\n",
    "                                    featcols = D.columns.tolist()\n",
    "#                                     #scores\n",
    "#                                     D['Bradykinesia'] = subj_score['Bradykinesia'][i]\n",
    "#                                     D['Tremor'] = subj_score['Tremor'][i]\n",
    "                                    #metadata\n",
    "#                                     D['Visit'] = subj_score.visit[i] \n",
    "                                    D['Task'] = task\n",
    "                                    D['Location'] = loc \n",
    "                                    D['Subject'] = subj\n",
    "                                    Data = pd.concat([Data,D]) #concatenate data from each visit\n",
    "                            else:\n",
    "                                continue\n",
    "                            \n",
    "        # temp file saving in case of errors in later subjects disables ability to save entire csv\n",
    "#         temp_filename = 'Data'+str(subj)+'.csv'\n",
    "#         new_folder = os.path.join(features_path,'Temp')\n",
    "#         Data.to_csv(os.path.join(new_folder,temp_filename))\n",
    "#         print('Saved temp Feature matrix after including subj %d'%subj)\n",
    "\n",
    "    cols = ['Subject','Task','Location']+ featcols #, 'Visit','Bradykinesia','Tremor']+ featcols\n",
    "    Data = Data[cols]    \n",
    "    print('\\nData matrix generated')\n",
    "    print(Data.shape)\n",
    "\n",
    "    #save data to feature matrix\n",
    "#     if symptom == None:\n",
    "#         symptom=''\n",
    "    saved_filename = 'DataFinal'+'.csv'#+symptom+'.csv'\n",
    "    if append and os.path.isfile(os.path.join(features_path,saved_filename)):\n",
    "        Data.to_csv(os.path.join(features_path,saved_filename),mode='a+',header=False) #append to existing\n",
    "        print('Appending to existing Feature matrix ' + str(os.path.join(features_path,saved_filename)))    \n",
    "\n",
    "    else:\n",
    "        Data.to_csv(os.path.join(features_path,saved_filename))\n",
    "        print('Feature matrix saved in ' + str(os.path.join(features_path,saved_filename)))\n",
    "    \n",
    "    t = time.time()\n",
    "    eltime = (t-t0)/60\n",
    "    print('Elapsed time = %.2f min'%eltime)\n",
    "\n",
    "    #     return Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
