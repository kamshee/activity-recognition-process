{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: clip_data() \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import os\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import pickle\n",
    "from itertools import product\n",
    "from scipy.stats import skew, kurtosis, pearsonr\n",
    "from scipy.signal import butter, welch, filtfilt, resample\n",
    "import time\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if platform.system() == 'Windows':\n",
    "#     if platform.release() == '7':\n",
    "path = r'//FS2.smpp.local\\RTO\\Inpatient Sensors -Stroke\\Data\\biostamp_data\\controls'\n",
    "folder_path = r'//FS2.smpp.local\\RTO\\Inpatient Sensors -Stroke\\Data\\biostamp_data'\n",
    "dict_path = r'//FS2.smpp.local\\RTO\\Inpatient Sensors -Stroke\\Data\\biostamp_data\\Data_dict'   \n",
    "#        features_path = r'X:\\CIS-PD Study\\FeatureMatrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Pickle file dict\n",
    "subj = 'HC02'\n",
    "f = open(os.path.join(dict_path, subj + 'dict.pkl'), 'rb')\n",
    "act_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bug in flatten\n",
    "\n",
    "# flatten clip_data nested dictionary and check contents\n",
    "tempdf = pd.DataFrame([(k1, k2, k3, v) for k1, k23v in clip_data.items()\n",
    "                   for k2, k3v in k23v.items()\n",
    "                   for k3, v in k3v.items()])\n",
    "# tempdf.columns = ['trial','sensor','data','dunno']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE\n",
    "# unstack nested dictionary\n",
    "df = pd.DataFrame([(k1, k2, k3, k4, v) for k1, k234v in act_dict.items()\n",
    "                           for k2, k34v in k234v.items()\n",
    "                           for k3, k4v in k34v.items()\n",
    "                           for k4, v in k4v.items()])\n",
    "df.columns = ['task','trial','location','sensor','rawdata']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try walking first - should have no problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "actdictaccgyr = act_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose task, location\n",
    "task = 'WALKING'\n",
    "loc = 'sacrum'\n",
    "# sensor = 'accel'\n",
    "# trial = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WALKING  sensortype = accel - trial 0\n",
      "clip len  8\n",
      "WALKING  sensortype = gyro - trial 0\n",
      "clip len  8\n",
      "WALKING  sensortype = accel - trial 1\n",
      "clip len  84\n",
      "WALKING  sensortype = gyro - trial 1\n",
      "clip len  84\n",
      "WALKING  sensortype = accel - trial 2\n",
      "clip len  14\n",
      "WALKING  sensortype = gyro - trial 2\n",
      "clip len  14\n",
      "WALKING  sensortype = accel - trial 3\n",
      "clip len  92\n",
      "WALKING  sensortype = gyro - trial 3\n",
      "clip len  92\n"
     ]
    }
   ],
   "source": [
    "#clipsize=0 extracts full recordings\n",
    "clip_data = gen_clips_merged(act_dict,task,loc,clipsize=10000,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                Accel X (g)  Accel Y (g)  Accel Z (g)\n",
       " Timestamp (ms)                                       \n",
       " 0                  0.010315     0.925002     0.245182\n",
       " 32                -0.009461     0.990311     0.212833\n",
       " 64                 0.023865     1.028641     0.190494\n",
       " 96                 0.015198     0.983719     0.186587\n",
       " 128               -0.083314     0.886671     0.190005\n",
       " 160               -0.169131     0.942824     0.200137\n",
       " 192               -0.067323     1.047440     0.211734\n",
       " 224                0.137514     1.080888     0.223331\n",
       " 256                0.048890     1.224323     0.244083\n",
       " 288               -0.126162     1.232136     0.310002\n",
       " 320               -0.026795     1.008377     0.438666\n",
       " 352                0.006653     0.829297     0.482124\n",
       " 384               -0.101991     0.726513     0.483589\n",
       " 416               -0.077821     0.989456     0.444648\n",
       " 448                0.073793     1.267414     0.373358\n",
       " 480                0.059876     0.995682     0.337346\n",
       " 512                0.066224     0.848707     0.350408\n",
       " 544                0.072938     0.834424     0.294255\n",
       " 576               -0.002869     0.849683     0.279728\n",
       " 608               -0.056580     0.864698     0.239933\n",
       " 640               -0.030091     0.882643     0.217105\n",
       " 672               -0.026429     0.893263     0.173404\n",
       " 704               -0.034363     0.901808     0.156802\n",
       " 736               -0.005310     1.000320     0.238102\n",
       " 768               -0.084291     1.091630     0.112245\n",
       " 800               -0.036316     1.212238     0.136660\n",
       " 832               -0.036439     1.246662     0.294987\n",
       " 864               -0.116640     1.049149     0.560006\n",
       " 896               -0.192081     0.924025     0.590402\n",
       " 928                0.098695     0.820508     0.476265\n",
       " ...                     ...          ...          ...\n",
       " 39054             -0.089784     1.487144     0.238468\n",
       " 39086             -0.167056     1.174273     0.517281\n",
       " 39119             -0.152407     0.831617     0.557321\n",
       " 39151              0.157412     0.770092     0.501045\n",
       " 39183              0.231632     1.078080     0.444282\n",
       " 39215             -0.221744     1.286213     0.348089\n",
       " 39247             -0.291081     0.894240     0.315862\n",
       " 39279             -0.129458     0.767773     0.296574\n",
       " 39311             -0.008606     0.784375     0.245670\n",
       " 39343              0.008118     0.841627     0.195010\n",
       " 39375             -0.009949     0.898756     0.199161\n",
       " 39407             -0.024109     0.841260     0.168276\n",
       " 39439             -0.066224     0.820264     0.158755\n",
       " 39471             -0.082216     0.894972     0.140322\n",
       " 39503              0.000916     1.058793     0.128481\n",
       " 39535              0.010559     1.281209     0.244205\n",
       " 39567             -0.074037     1.349935     0.267277\n",
       " 39599             -0.094057     1.107500     0.530587\n",
       " 39631              0.067933     0.914504     0.550607\n",
       " 39663             -0.080018     0.824170     0.509346\n",
       " 39695             -0.113100     1.109453     0.410346\n",
       " 39727              0.112734     1.296101     0.336370\n",
       " 39759              0.153262     0.937819     0.308415\n",
       " 39791              0.088441     0.773022     0.303532\n",
       " 39823              0.059266     0.780713     0.215640\n",
       " 39855              0.003113     0.824414     0.198550\n",
       " 39887             -0.084779     0.859693     0.201968\n",
       " 39919             -0.081361     0.908644     0.177676\n",
       " 39951             -0.042664     0.883253     0.151186\n",
       " 39983             -0.034119     0.887404     0.139223\n",
       " \n",
       " [2500 rows x 3 columns]]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_data[0]['accel']['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matlab code of features\n",
    "function [fvec, flab] = getFeatures(acc)\n",
    "% INPUT acc - 3 x n vector: 1-3 are xyz accelerations for n data points\n",
    "\n",
    "S = acc(1:3,:); %matrix of xyz acceleration data (excludes time data)\n",
    "\n",
    "fvec = []; %stores features\n",
    "flab = {}; %stores names of features\n",
    "axes = {'x','y','z'};\n",
    "\n",
    "%% Features for Each Axis (Time Domain)\n",
    "for i=1:size(S,1)\n",
    "# Done\n",
    "    %Mean \n",
    "    fvec = [fvec nanmean(S(i,:))]; flab = [flab; [axes{i} '-mean']];\n",
    "# Done\n",
    "    %Range of values\n",
    "    fvec = [fvec range(S(i,:))]; flab = [flab; [axes{i} '-range']];\n",
    "# Added \n",
    "    %Interquartile Range\n",
    "    fvec = [fvec iqr(S(i,:))]; flab = [flab; [axes{i} '-IQR']];    \n",
    "###########\n",
    "    %Histogram of the z-score values\n",
    "# python equivalent?\n",
    "    zvals = -2:1:2;\n",
    "    histvec = histc((S(i,:)-nanmean(S(i,:))/nanstd(S(i,:))),zvals);\n",
    "    histvec = histvec(1:end-1); % removing the last data point which counts how many values match exactly 3. (nonsense)\n",
    "    fvec = [fvec histvec]; \n",
    "    for j=1:length(histvec),\n",
    "        flab = [flab; [axes{i} sprintf('-hist%d',zvals(j))]];\n",
    "    end\n",
    "# Done    \n",
    "    %Std (2nd moment)\n",
    "    fvec = [fvec nanstd(S(i,:))];  flab = [flab; [axes{i} '-std']];\n",
    "# Done and Done    \n",
    "    %Skewness + Kurtosis (3rd and 4th moments)\n",
    "    if nanstd(S(i,:)) == 0\n",
    "        X = S(i,:); N = length(X);\n",
    "        s = 1/N*sum((X-mean(X)).^3)/( sqrt(1/N*sum((X-mean(X)).^2)) + eps )^3; %skewness\n",
    "        k = 1/N*sum((X-mean(X)).^4)/( 1/N*sum((X-mean(X)).^2) + eps )^2; %kurtosis\n",
    "        fvec = [fvec s]; flab = [flab; [axes{i} '-skew']];\n",
    "        fvec = [fvec k]; flab = [flab; [axes{i} '-kurt']];\n",
    "    else\n",
    "        fvec = [fvec skewness(S(i,:))]; flab = [flab; [axes{i} '-skew']];\n",
    "        fvec = [fvec kurtosis(S(i,:))]; flab = [flab; [axes{i} '-kurt']];\n",
    "    end\n",
    "# Added     \n",
    "    %Mean of differences\n",
    "    fvec = [fvec nanmean(diff(S(i,:)))]; flab = [flab; [axes{i} '-mean diff']];\n",
    "# Added    \n",
    "    %Std of differences (2nd moment)\n",
    "    fvec = [fvec nanstd(diff(S(i,:)))]; flab = [flab; [axes{i} '-std diff']];\n",
    "# Added both    \n",
    "    %Skewness + Kurtosis of differences (3rd and 4th moments)\n",
    "# Did NOT add condition\n",
    "    if nanstd(diff(S(i,:))) == 0\n",
    "        X = diff(S(i,:)); N = length(X);\n",
    "        s = 1/N*sum((X-mean(X)).^3)/( sqrt(1/N*sum((X-mean(X)).^2)) + eps )^3; %skewness\n",
    "        k = 1/N*sum((X-mean(X)).^4)/( 1/N*sum((X-mean(X)).^2) + eps )^2; %kurtosis \n",
    "        fvec = [fvec s]; flab = [flab; [axes{i} '-skew diff']];\n",
    "        fvec = [fvec k]; flab = [flab; [axes{i} '-kurt diff']];\n",
    "    else\n",
    "        fvec = [fvec skewness(diff(S(i,:)))]; flab = [flab; [axes{i} '-skew diff']];\n",
    "        fvec = [fvec kurtosis(diff(S(i,:)))]; flab = [flab; [axes{i} '-kurt diff']];\n",
    "    end\n",
    "end\n",
    "\n",
    "# ? \n",
    "%% Features Across All Axes (Time Domain)\n",
    "%Mean of squares\n",
    "fvec = [fvec nanmean(nanmean(S.^2))];\n",
    "flab = [flab; 'mean of squares'];\n",
    "\n",
    "# check calculation\n",
    "%Normalize values (divided by acc norm)\n",
    "S2=S./(ones(size(S,1),1)*sqrt(nansum(S.^2)));\n",
    "\n",
    "# added\n",
    "%Cross products of normalized values\n",
    "fvec = [fvec nanmean(S2(1,:).*S2(2,:))]; flab = [flab; 'CrossProd norm xy'];\n",
    "fvec = [fvec nanmean(S2(1,:).*S2(3,:))]; flab = [flab; 'CrossProd norm xz'];\n",
    "fvec = [fvec nanmean(S2(2,:).*S2(3,:))]; flab = [flab; 'CrossProd norm yz'];\n",
    "fvec = [fvec abs(nanmean(S2(1,:).*S2(2,:)))]; flab = [flab; 'abs CrossProd xy']; \n",
    "fvec = [fvec abs(nanmean(S2(1,:).*S2(3,:)))]; flab = [flab; 'abs CrossProd xz'];\n",
    "fvec = [fvec abs(nanmean(S2(2,:).*S2(3,:)))]; flab = [flab; 'abs CrossProd yz'];\n",
    "\n",
    "# added\n",
    "%Cross products of raw acceleration values\n",
    "fvec = [fvec nanmean(S(1,:).*S(2,:))]; flab = [flab; 'CrossProd xy'];\n",
    "fvec = [fvec nanmean(S(1,:).*S(3,:))]; flab = [flab; 'CrossProd xz'];\n",
    "fvec = [fvec nanmean(S(2,:).*S(3,:))]; flab = [flab; 'CrossProd yz'];\n",
    "fvec = [fvec abs(nanmean(S(1,:).*S(2,:)))]; flab = [flab; 'abs CrossProd xy'];\n",
    "fvec = [fvec abs(nanmean(S(1,:).*S(3,:)))]; flab = [flab; 'abs CrossProd xz'];\n",
    "fvec = [fvec abs(nanmean(S(2,:).*S(3,:)))]; flab = [flab; 'abs CrossProd yz'];\n",
    "\n",
    "# Done\n",
    "%Correlation coefficients r(x,y)\n",
    "fvec = [fvec corr(S(1,:)',S(2,:)')]; flab = [flab; 'corr coeff xy'];\n",
    "fvec = [fvec corr(S(2,:)',S(3,:)')]; flab = [flab; 'corr coeff yz'];\n",
    "fvec = [fvec corr(S(1,:)',S(3,:)')]; flab = [flab; 'corr coeff xz'];\n",
    "\n",
    "# added\n",
    "%Sum of xyz std\n",
    "fvec = [fvec sum([std(S(1,:)) std(S(2,:)) std(S(3,:))])]; flab = [flab; 'std_sum'];\n",
    "\n",
    "# added\n",
    "% %Linear Fit\n",
    "% ws = warning('off','all'); %Turn off warning\n",
    "% p_xy = polyfit(S(1,:),S(2,:),1);\n",
    "% p_xz = polyfit(S(1,:),S(3,:),1);\n",
    "% p_yz = polyfit(S(2,:),S(3,:),1);\n",
    "% fvec = [fvec p_xy(1)]; flab = [flab; 'slope xy'];\n",
    "% fvec = [fvec p_xz(1)]; flab = [flab; 'slope xz'];\n",
    "% fvec = [fvec p_yz(1)]; flab = [flab; 'slope yz'];\n",
    "% warning(ws); %Turn warning back on\n",
    "\n",
    "%% Frequency Domain Processing (High Pass + Power Spectra)\n",
    "filtered = cell(3,1);\n",
    "PSD_welch = cell(3,1);\n",
    "f_welch = cell(3,1);\n",
    "fc = 0.2; %cutoff frequency (Hz)\n",
    "fs = 30;\n",
    "f_nyq = fs/2;\n",
    "\n",
    "%High Pass Filter\n",
    "for ii = 1:3\n",
    "    [b, a] = butter(2,(fc*pi)/f_nyq,'high');\n",
    "    filtered{ii} = filter(b,a,S(ii,:)); \n",
    "end\n",
    "\n",
    "%Power Spectra\n",
    "for ii = 1:3\n",
    "    win_size = ceil(length(filtered{ii})/2);\n",
    "    [PSD_welch{ii}, f_welch{ii}] = pwelch(filtered{ii}, win_size, [], [], fs);\n",
    "end\n",
    "\n",
    "%% Features for Each Axis (Frequency Domain)\n",
    "for ii = 1:3\n",
    "\n",
    "# Done\n",
    "    %Mean \n",
    "    fvec = [fvec nanmean(PSD_welch{ii})]; flab = [flab; [axes{ii} '-mean (PSD)']];\n",
    "# Done    \n",
    "    %Std (2nd moment)\n",
    "    fvec = [fvec nanstd(PSD_welch{ii})];  flab = [flab; [axes{ii} '-std (PSD)']];\n",
    "# Done    \n",
    "    %Skewness + Kurtosis (3rd and 4th moments)\n",
    "    if nanstd(PSD_welch{ii}) == 0\n",
    "        X = PSD_welch{ii}; N = length(X);\n",
    "        s = 1/N*sum((X-mean(X)).^3)/( sqrt(1/N*sum((X-mean(X)).^2)) + eps )^3; %skewness\n",
    "        k = 1/N*sum((X-mean(X)).^4)/( 1/N*sum((X-mean(X)).^2) + eps )^2; %kurtosis\n",
    "        fvec = [fvec s]; flab = [flab; [axes{ii} '-skew (PSD)']];\n",
    "        fvec = [fvec k]; flab = [flab; [axes{ii} '-kurt (PSD)']];\n",
    "    else\n",
    "        fvec = [fvec skewness(PSD_welch{ii})]; flab = [flab; [axes{ii} '-skew (PSD)']];\n",
    "        fvec = [fvec kurtosis(PSD_welch{ii})]; flab = [flab; [axes{ii} '-kurt (PSD)']];\n",
    "    end\n",
    "# ???    \n",
    "    %Mean Power for 0.5 Hz Intervals\n",
    "    bins = [0:0.5:10]; %0-10 Hz with bins for every 0.5 Hz\n",
    "    N = length(bins)-1;\n",
    "    bin_ind = [1; zeros(N,1)];\n",
    "    %PSD_welch_norm = PSD_welch{ii}/max(PSD_welch{ii});\n",
    "    PSD_welch_norm = PSD_welch{ii};\n",
    "    bin_val = zeros(N,1);\n",
    "    for jj = 2:length(bins) %ignore 0 Hz in bins vector\n",
    "        freq_bin = bins(jj);\n",
    "        for zz = 2:length(f_welch{ii})\n",
    "           if ((f_welch{ii}(zz-1) < freq_bin) && (f_welch{ii}(zz) > freq_bin)) || f_welch{ii}(zz) == freq_bin\n",
    "               bin_ind(jj) = zz;\n",
    "           end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    for kk = 2:length(bins)\n",
    "        bin_val(kk) = mean(PSD_welch_norm(bin_ind(kk-1):bin_ind(kk)));\n",
    "    end\n",
    "    fvec = [fvec bin_val(2:end)'];\n",
    "    for jj = 2:length(bin_val)\n",
    "        flab = [flab; [axes{ii} sprintf('_bin_%d',bins(jj))]];\n",
    "    end\n",
    "end\n",
    "return\n",
    "\n",
    "# Not sure on this???\n",
    "%% Features Across All Axes (Frequency Domain)\n",
    "%Sum of std\n",
    "fvec = [fvec sum([std(PSD_welch{1}) std(PSD_welch{2}) std(PSD_welch{3})])]; flab = [flab; 'std_sum'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_clips_merged(act_dict,task,location,clipsize=10000,overlap=0.9,verbose=False,startTS=0,endTS=1,\n",
    "              len_tol=0.95,resample=False):\n",
    "    \"\"\"\n",
    "    Extract clips and merge into 1 clip for accelerometer and gyro data (allows selecting start and end fraction)\n",
    "    len_tol is the % of the intended clipsize below which clip is not used\n",
    "    \n",
    "    :param clipsize 10000 = 10 sec\n",
    "    :param overlap=0.9 for 90% overlap b/n clips\n",
    "    :param len_tol=1.0, want complete 10 sec clips\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    clip_data = {} #the dictionary with clips\n",
    "\n",
    "    for trial in act_dict[task].keys():\n",
    "        clip_data[trial] = {}\n",
    "\n",
    "        for s in ['accel','gyro']:\n",
    "\n",
    "            if verbose:\n",
    "                print(task,' sensortype = %s - trial %d'%(s,trial))\n",
    "            #create clips and store in a list\n",
    "            rawdata = act_dict[task][trial][location][s]\n",
    "            if rawdata.empty is True: #skip if no data for current sensor\n",
    "                continue\n",
    "            #reindex time (relative to start)\n",
    "            idx = rawdata.index\n",
    "            idx = idx-idx[0]\n",
    "            rawdata.index = idx\n",
    "            #choose to create clips only on a fraction of the data (0<[startTS,endTS]<1)\n",
    "            if (startTS > 0) | (endTS < 1):\n",
    "                rawdata = rawdata.iloc[round(startTS*len(rawdata)):round(endTS*len(rawdata)),:]\n",
    "                #reindex time (relative to start)\n",
    "                idx = rawdata.index\n",
    "                idx = idx-idx[0]\n",
    "                rawdata.index = idx\n",
    "            #create clips data\n",
    "            deltat = np.median(np.diff(rawdata.index))\n",
    "            clips = []\n",
    "            #use entire recording\n",
    "            if clipsize == 0:\n",
    "                clips.append(rawdata)\n",
    "            #take clips\n",
    "            else:\n",
    "                idx = np.arange(0,rawdata.index[-1],clipsize*(1-overlap))\n",
    "                for i in idx:\n",
    "                    c = rawdata[(rawdata.index>=i) & (rawdata.index<i+clipsize)]\n",
    "                    #keep/append clips that are 10 sec, else discard those that don't meet length\n",
    "                    #tolerance\n",
    "                    ## clip length tolerance > 9.5 sec\n",
    "                    if len(c) > len_tol*int(clipsize/deltat):\n",
    "                        # try concat instead of append to make one list\n",
    "                        # check index, if increases like 0, 32, 64, etc then great, otherwise\n",
    "                        # reindex c before extending?\n",
    "                        clips.append(c)\n",
    "\n",
    "            # merge all clips into one\n",
    "            # cycle through each list element, reindex\n",
    "            if len(clips)>1:\n",
    "#                 finalclip = []\n",
    "                for x in range(len(clips)):\n",
    "                    if x==0:\n",
    "                        clips.append(clips[0])\n",
    "                    else:\n",
    "                        # reindex\n",
    "                        idx2 = clips[x].index\n",
    "                        idx2 = idx2-idx2[0]+32+clips[x-1].index[-1]\n",
    "                        clips[x].index = idx2\n",
    "                        # merge into list of dataframes\n",
    "                        clips.append(clips[x])\n",
    "    #####################\n",
    "            print('clip len ', len(clips))\n",
    "            # merge into one dataframe\n",
    "            oneclip = pd.concat(clips)\n",
    "            # reset clips\n",
    "            clips = []\n",
    "            clips.append(oneclip)\n",
    "    \n",
    "            #store clip length\n",
    "            #store the length of each clip\n",
    "            clip_len = [clips[c].index[-1]-clips[c].index[0] for c in range(len(clips))] \n",
    "            #assemble in dict\n",
    "            clip_data[trial][s] = {'data':clips, 'clip_len':clip_len}\n",
    "\n",
    "    return clip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different col names for acc and gyro\n",
    "def feature_extraction2(clip_data):\n",
    "    \"\"\"\n",
    "    Extract features from both sensors (accel and gyro) for current clips and trials\n",
    "    Input: dictionary of clips from each subject\n",
    "    Output: feature matrix from all clips from given subject and scores for each clip\n",
    "    \"\"\"\n",
    "    \n",
    "    features_list = ['RMSX','RMSY','RMSZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ','xcor_peakXY','xcorr_peakXZ','xcorr_peakYZ',\n",
    "                    'xcorr_lagXY','xcorr_lagXZ','xcorr_lagYZ','Dom_freq','Pdom_rel','PSD_mean','PSD_std','PSD_skew',\n",
    "                    'PSD_kur','jerk_mean','jerk_std','jerk_skew','jerk_kur','Sen_X','Sen_Y','Sen_Z']\n",
    "#                     ,'RMS_mag','range_mag',\n",
    "#                     'mean_mag','var_mag','skew_mag','kurt_mag','Sen_mag']\n",
    "    acclist = [s + '_acc' for s in features_list]\n",
    "    gyrlist = [s + '_gyr' for s in features_list]\n",
    "\n",
    "    for trial in clip_data.keys():\n",
    "\n",
    "        for sensor in clip_data[trial].keys():\n",
    "\n",
    "            #cycle through all clips for current trial and save dataframe of features for current trial and sensor\n",
    "            features = []\n",
    "            for c in range(len(clip_data[trial][sensor]['data'])):\n",
    "                rawdata = clip_data[trial][sensor]['data'][c]\n",
    "            \n",
    "                #acceleration magnitude\n",
    "                rawdata_wmag = rawdata.copy()\n",
    "                rawdata_wmag['Accel_Mag']=np.sqrt((rawdata**2).sum(axis=1))\n",
    "\n",
    "                #extract features on current clip\n",
    "\n",
    "                #Root mean square of signal on each axis\n",
    "                N = len(rawdata)\n",
    "                RMS = 1/N*np.sqrt(np.asarray(np.sum(rawdata**2,axis=0)))\n",
    "\n",
    "        #         RMS_mag = 1/N*np.sqrt(np.sum(rawdata_wmag['Accel_Mag']**2,axis=0))\n",
    "\n",
    "                #range on each axis\n",
    "                min_xyz = np.min(rawdata,axis=0)\n",
    "                max_xyz = np.max(rawdata,axis=0)\n",
    "                r = np.asarray(max_xyz-min_xyz)\n",
    "\n",
    "        #         r_mag = np.max(rawdata_wmag['Accel_Mag']) - np.min(rawdata_wmag['Accel_Mag'])\n",
    "\n",
    "                #Moments on each axis\n",
    "                mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "                var = np.asarray(np.std(rawdata,axis=0))\n",
    "                sk = skew(rawdata)\n",
    "                kurt = kurtosis(rawdata)\n",
    "\n",
    "        #         mean_mag = np.mean(rawdata_wmag['Accel_Mag'])\n",
    "        #         var_mag = np.std(rawdata_wmag['Accel_Mag'])\n",
    "        #         sk_mag = skew(rawdata_wmag['Accel_Mag'])\n",
    "        #         kurt_mag = kurtosis(rawdata_wmag['Accel_Mag'])\n",
    "\n",
    "                #Cross-correlation between axes pairs\n",
    "                xcorr_xy = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,1],mode='same')\n",
    "                # xcorr_xy = xcorr_xy/np.abs(np.sum(xcorr_xy)) #normalize values\n",
    "                xcorr_peak_xy = np.max(xcorr_xy)\n",
    "                xcorr_lag_xy = (np.argmax(xcorr_xy))/len(xcorr_xy) #normalized lag\n",
    "\n",
    "                xcorr_xz = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_xz = xcorr_xz/np.abs(np.sum(xcorr_xz)) #normalize values\n",
    "                xcorr_peak_xz = np.max(xcorr_xz)\n",
    "                xcorr_lag_xz = (np.argmax(xcorr_xz))/len(xcorr_xz)\n",
    "\n",
    "                xcorr_yz = np.correlate(rawdata.iloc[:,1],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_yz = xcorr_yz/np.abs(np.sum(xcorr_yz)) #normalize values\n",
    "                xcorr_peak_yz = np.max(xcorr_yz)\n",
    "                xcorr_lag_yz = (np.argmax(xcorr_yz))/len(xcorr_yz)\n",
    "\n",
    "                #pack xcorr features\n",
    "                xcorr_peak = np.array([xcorr_peak_xy,xcorr_peak_xz,xcorr_peak_yz])\n",
    "                xcorr_lag = np.array([xcorr_lag_xy,xcorr_lag_xz,xcorr_lag_yz])\n",
    "\n",
    "                #Dominant freq and relative magnitude (on acc magnitude)\n",
    "                Pxx = power_spectra_welch(rawdata_wmag,fm=0,fM=10)\n",
    "                domfreq = np.asarray([Pxx.iloc[:,-1].idxmax()])\n",
    "                Pdom_rel = Pxx.loc[domfreq].iloc[:,-1].values/Pxx.iloc[:,-1].sum() #power at dominant freq rel to total\n",
    "\n",
    "                #moments of PSD\n",
    "                Pxx_moments = np.array([np.nanmean(Pxx.values),np.nanstd(Pxx.values),skew(Pxx.values),kurtosis(Pxx.values)])\n",
    "\n",
    "                #moments of jerk magnitude\n",
    "                jerk = rawdata_wmag['Accel_Mag'].diff().values\n",
    "                jerk_moments = np.array([np.nanmean(jerk),np.nanstd(jerk),skew(jerk[~np.isnan(jerk)]),kurtosis(jerk[~np.isnan(jerk)])])\n",
    "\n",
    "                #sample entropy raw data (magnitude) and FFT\n",
    "                sH_raw = []; sH_fft = []\n",
    "\n",
    "                for a in range(3):\n",
    "                    x = rawdata.iloc[:,a]\n",
    "                    n = len(x) #number of samples in clip\n",
    "                    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "                    sH_raw.append(nolds.sampen(x)) #samp entr raw data\n",
    "                    #for now disable SH on fft\n",
    "                    # f,Pxx_den = welch(x,Fs,nperseg=min(256,n/4))\n",
    "                    # sH_fft.append(nolds.sampen(Pxx_den)) #samp entr fft\n",
    "\n",
    "                sH_mag = nolds.sampen(rawdata_wmag['Accel_Mag'])\n",
    "\n",
    "                #Assemble features in array\n",
    "        #         Y = np.array([RMS_mag,r_mag,mean_mag,var_mag,sk_mag,kurt_mag,sH_mag])\n",
    "                X = np.concatenate((RMS,r,mean,var,sk,kurt,xcorr_peak,xcorr_lag,domfreq,Pdom_rel,Pxx_moments,jerk_moments,sH_raw)) #,Y))\n",
    "                features.append(X)\n",
    "\n",
    "            F = np.asarray(features) #feature matrix for all clips from current trial\n",
    "        #     clip_data['features'] = pd.DataFrame(data=F,columns=features_list,dtype='float32')\n",
    "            if sensor == 'accel': columns_list = acclist\n",
    "            else: columns_list = gyrlist\n",
    "            try:\n",
    "                clip_data[trial][sensor]['features'] = pd.DataFrame(data=F,columns=columns_list,dtype='float32')\n",
    "            except:\n",
    "                print('Empty data in feature_extraction2')\n",
    "                continue\n",
    "#     return clip_data #not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregates acc and gyro features, combines trial\n",
    "# Need to loop through all task and locations and make one dataframe\n",
    "def aggregateAccGyroTrial(clip_data):\n",
    "    \"\"\"Take a clip of task/location with extracted ACC/GYR features, \n",
    "    and merge all trials into one dataframe.\n",
    "    \n",
    "    :param clip_data \n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for trial in clip_data.keys():\n",
    "#         trialdf = pd.DataFrame({'trial':[trial]})\n",
    "        \n",
    "        try: acc_df = clip_data[trial]['accel']['features']\n",
    "        except: \n",
    "            print('no accel data in trial ',trial)\n",
    "            acc_df = pd.DataFrame()\n",
    "            continue\n",
    "        try: gyr_df = clip_data[trial]['gyro']['features']\n",
    "        except: \n",
    "            print('no gyro data in trial ',trial)\n",
    "            gyr_df = pd.DataFrame()\n",
    "            continue\n",
    "#         print('gyr feat',gyr_df)\n",
    "#         tempdf.insert(0, 'task', task)\n",
    "#         trialdf = pd.concat([trialdf, acc_df, gyr_df], axis=1)\n",
    "        trialdf = pd.concat([acc_df, gyr_df], axis=1)\n",
    "        trialdf.insert(0, 'trial', trial)\n",
    "        \n",
    "        df = pd.concat([df,trialdf], ignore_index=True) # 0 prob by default\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loop through act_dict\n",
    "- task\n",
    "- location\n",
    "clip_data = gen_clips(act_dict,task,loc,clipsize=10000,verbose=True)\n",
    "feature_extraction2(clip_data)\n",
    "aggregateAccGyroTrial(clip_data)\n",
    "append clip_data into final df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = ['tibialis_anterior_left', 'gastrocnemius_right', 'sacrum', 'distal_lateral_shank_right', \n",
    "             'tibialis_anterior_right', 'posterior_forearm_right', 'bicep_right', 'rectus_femoris_left', \n",
    "             'biceps_femoris_right', 'posterior_forearm_left', 'biceps_femoris_left', 'gastrocnemius_left', \n",
    "             'bicep_left', 'medial_chest', 'distal_lateral_shank_left', 'rectus_femoris_right']\n",
    "finaldf = pd.DataFrame()\n",
    "for task in act_dict.keys():\n",
    "    for location in locations:\n",
    "        clip_data = gen_clips(act_dict,task,loc,clipsize=10000,verbose=True)\n",
    "        feature_extraction2(clip_data)\n",
    "        tempdf = aggregateAccGyroTrial(clip_data)\n",
    "        finaldf = pd.concat([finaldf,tempdf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try aggregating with clip and feat extraction functions\n",
    "- different approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Libraries\n",
    "import os\n",
    "import platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import pickle\n",
    "from itertools import product\n",
    "from scipy.stats import skew, kurtosis, pearsonr\n",
    "from scipy.signal import butter, welch, filtfilt, resample\n",
    "import time\n",
    "import re\n",
    "import copy\n",
    "import nolds\n",
    "\n",
    "from PreprocessFcns import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = r'//FS2.smpp.local\\RTO\\Inpatient Sensors -Stroke\\Data\\biostamp_data\\controls'\n",
    "folder_path = r'//FS2.smpp.local\\RTO\\Inpatient Sensors -Stroke\\Data\\biostamp_data'\n",
    "dict_path = r'//FS2.smpp.local\\RTO\\Inpatient Sensors -Stroke\\Data\\biostamp_data\\Data_dict'\n",
    "features_path = r'//FS2.smpp.local\\RTO\\Inpatient Sensors -Stroke\\Data\\biostamp_data\\FeatureMatrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified Andrew's function\n",
    "def gen_clips(act_dict,task,location,clipsize=10000,overlap=0.9,verbose=False,startTS=0,endTS=1,\n",
    "              len_tol=1.0,resample=False):\n",
    "    \"\"\"\n",
    "    Extract clips for accelerometer and gyro data (allows selecting start and end fraction)\n",
    "    len_tol is the % of the intended clipsize below which clip is not used\n",
    "    \n",
    "    Default arg changes:\n",
    "    - changed clipsize=5000 to 10000 for 10 sec clips\n",
    "    - changed overlap=0 to 0.9 for 90% overlap b/n clips\n",
    "    - changed len_tol=0.8 to 1.0, discard clips less than 10 sec\n",
    "\n",
    "    Understand the following args:\n",
    "    startTS\n",
    "    endTS\n",
    "    \"\"\"\n",
    "    \n",
    "    clip_data = {} #the dictionary with clips\n",
    "\n",
    "    for trial in act_dict[task].keys():\n",
    "        clip_data[trial] = {}\n",
    "\n",
    "        for s in ['accel','gyro']:\n",
    "\n",
    "            if verbose:\n",
    "                print(task,' sensortype = %s - trial %d'%(s,trial))\n",
    "            #create clips and store in a list\n",
    "            rawdata = act_dict[task][trial][location][s]\n",
    "            if rawdata.empty is True: #skip if no data for current sensor\n",
    "                continue\n",
    "            #reindex time (relative to start)\n",
    "            idx = rawdata.index\n",
    "            idx = idx-idx[0]\n",
    "            rawdata.index = idx\n",
    "            #choose to create clips only on a fraction of the data (0<[startTS,endTS]<1)\n",
    "            if (startTS > 0) | (endTS < 1):\n",
    "                rawdata = rawdata.iloc[round(startTS*len(rawdata)):round(endTS*len(rawdata)),:]\n",
    "                #reindex time (relative to start)\n",
    "                idx = rawdata.index\n",
    "                idx = idx-idx[0]\n",
    "                rawdata.index = idx\n",
    "            #create clips data\n",
    "            deltat = np.median(np.diff(rawdata.index))\n",
    "            clips = []\n",
    "            #use entire recording\n",
    "            if clipsize == 0:\n",
    "                clips.append(rawdata)\n",
    "            #take clips\n",
    "            else:\n",
    "                idx = np.arange(0,rawdata.index[-1],clipsize*(1-overlap))\n",
    "                for i in idx:\n",
    "                    c = rawdata[(rawdata.index>=i) & (rawdata.index<i+clipsize)]\n",
    "                    #keep/append clips that are 10 sec, else discard those that don't meet length\n",
    "                    #tolerance\n",
    "                    ## changed > to >= so it will keep clip>=10 sec\n",
    "                    if len(c) >= len_tol*int(clipsize/deltat):\n",
    "                        clips.append(c)\n",
    "\n",
    "            #store clip length\n",
    "            #store the length of each clip\n",
    "# removed clip_len            \n",
    "#             clip_len = [clips[c].index[-1]-clips[c].index[0] for c in range(len(clips))] \n",
    "            #assemble in dict\n",
    "            clip_data[trial][s] = {'data':clips}#, 'clip_len':clip_len}\n",
    "\n",
    "    return clip_data\n",
    "\n",
    "# used function \n",
    "def feature_extraction(clip_data):\n",
    "    \"\"\"\n",
    "    Extract features from both sensors (accel and gyro) for current clips and trials\n",
    "    Input: dictionary of clips from each subject\n",
    "    Output: feature matrix from all clips from given subject and scores for each clip\n",
    "    \"\"\"\n",
    "    \n",
    "    features_list = ['RMSX','RMSY','RMSZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ','xcor_peakXY','xcorr_peakXZ','xcorr_peakYZ',\n",
    "                    'xcorr_lagXY','xcorr_lagXZ','xcorr_lagYZ','Dom_freq','Pdom_rel','PSD_mean','PSD_std','PSD_skew',\n",
    "                    'PSD_kur','jerk_mean','jerk_std','jerk_skew','jerk_kur','Sen_X','Sen_Y','Sen_Z']\n",
    "#                     ,'RMS_mag','range_mag',\n",
    "#                     'mean_mag','var_mag','skew_mag','kurt_mag','Sen_mag']\n",
    "\n",
    "    for trial in clip_data.keys():\n",
    "\n",
    "        for sensor in clip_data[trial].keys():\n",
    "\n",
    "            #cycle through all clips for current trial and save dataframe of features for current trial and sensor\n",
    "            features = []\n",
    "            for c in range(len(clip_data[trial][sensor]['data'])):\n",
    "                rawdata = clip_data[trial][sensor]['data'][c]\n",
    "            \n",
    "                #acceleration magnitude\n",
    "                rawdata_wmag = rawdata.copy()\n",
    "                rawdata_wmag['Accel_Mag']=np.sqrt((rawdata**2).sum(axis=1))\n",
    "\n",
    "                #extract features on current clip\n",
    "\n",
    "                #Root mean square of signal on each axis\n",
    "                N = len(rawdata)\n",
    "                RMS = 1/N*np.sqrt(np.asarray(np.sum(rawdata**2,axis=0)))\n",
    "\n",
    "        #         RMS_mag = 1/N*np.sqrt(np.sum(rawdata_wmag['Accel_Mag']**2,axis=0))\n",
    "\n",
    "                #range on each axis\n",
    "                min_xyz = np.min(rawdata,axis=0)\n",
    "                max_xyz = np.max(rawdata,axis=0)\n",
    "                r = np.asarray(max_xyz-min_xyz)\n",
    "\n",
    "        #         r_mag = np.max(rawdata_wmag['Accel_Mag']) - np.min(rawdata_wmag['Accel_Mag'])\n",
    "\n",
    "                #Moments on each axis\n",
    "                mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "                var = np.asarray(np.std(rawdata,axis=0))\n",
    "                sk = skew(rawdata)\n",
    "                kurt = kurtosis(rawdata)\n",
    "\n",
    "        #         mean_mag = np.mean(rawdata_wmag['Accel_Mag'])\n",
    "        #         var_mag = np.std(rawdata_wmag['Accel_Mag'])\n",
    "        #         sk_mag = skew(rawdata_wmag['Accel_Mag'])\n",
    "        #         kurt_mag = kurtosis(rawdata_wmag['Accel_Mag'])\n",
    "\n",
    "                #Cross-correlation between axes pairs\n",
    "                xcorr_xy = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,1],mode='same')\n",
    "                # xcorr_xy = xcorr_xy/np.abs(np.sum(xcorr_xy)) #normalize values\n",
    "                xcorr_peak_xy = np.max(xcorr_xy)\n",
    "                xcorr_lag_xy = (np.argmax(xcorr_xy))/len(xcorr_xy) #normalized lag\n",
    "\n",
    "                xcorr_xz = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_xz = xcorr_xz/np.abs(np.sum(xcorr_xz)) #normalize values\n",
    "                xcorr_peak_xz = np.max(xcorr_xz)\n",
    "                xcorr_lag_xz = (np.argmax(xcorr_xz))/len(xcorr_xz)\n",
    "\n",
    "                xcorr_yz = np.correlate(rawdata.iloc[:,1],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_yz = xcorr_yz/np.abs(np.sum(xcorr_yz)) #normalize values\n",
    "                xcorr_peak_yz = np.max(xcorr_yz)\n",
    "                xcorr_lag_yz = (np.argmax(xcorr_yz))/len(xcorr_yz)\n",
    "\n",
    "                #pack xcorr features\n",
    "                xcorr_peak = np.array([xcorr_peak_xy,xcorr_peak_xz,xcorr_peak_yz])\n",
    "                xcorr_lag = np.array([xcorr_lag_xy,xcorr_lag_xz,xcorr_lag_yz])\n",
    "\n",
    "                #Dominant freq and relative magnitude (on acc magnitude)\n",
    "                Pxx = power_spectra_welch(rawdata_wmag,fm=0,fM=10)\n",
    "                domfreq = np.asarray([Pxx.iloc[:,-1].idxmax()])\n",
    "                Pdom_rel = Pxx.loc[domfreq].iloc[:,-1].values/Pxx.iloc[:,-1].sum() #power at dominant freq rel to total\n",
    "\n",
    "                #moments of PSD\n",
    "                Pxx_moments = np.array([np.nanmean(Pxx.values),np.nanstd(Pxx.values),skew(Pxx.values),kurtosis(Pxx.values)])\n",
    "\n",
    "                #moments of jerk magnitude\n",
    "                jerk = rawdata_wmag['Accel_Mag'].diff().values\n",
    "                jerk_moments = np.array([np.nanmean(jerk),np.nanstd(jerk),skew(jerk[~np.isnan(jerk)]),kurtosis(jerk[~np.isnan(jerk)])])\n",
    "\n",
    "                #sample entropy raw data (magnitude) and FFT\n",
    "                sH_raw = []; sH_fft = []\n",
    "\n",
    "                for a in range(3):\n",
    "                    x = rawdata.iloc[:,a]\n",
    "                    n = len(x) #number of samples in clip\n",
    "                    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "                    sH_raw.append(nolds.sampen(x)) #samp entr raw data\n",
    "                    #for now disable SH on fft\n",
    "                    # f,Pxx_den = welch(x,Fs,nperseg=min(256,n/4))\n",
    "                    # sH_fft.append(nolds.sampen(Pxx_den)) #samp entr fft\n",
    "\n",
    "                sH_mag = nolds.sampen(rawdata_wmag['Accel_Mag'])\n",
    "\n",
    "                #Assemble features in array\n",
    "        #         Y = np.array([RMS_mag,r_mag,mean_mag,var_mag,sk_mag,kurt_mag,sH_mag])\n",
    "                X = np.concatenate((RMS,r,mean,var,sk,kurt,xcorr_peak,xcorr_lag,domfreq,Pdom_rel,Pxx_moments,jerk_moments,sH_raw)) #,Y))\n",
    "                features.append(X)\n",
    "\n",
    "            F = np.asarray(features) #feature matrix for all clips from current trial\n",
    "        #     clip_data['features'] = pd.DataFrame(data=F,columns=features_list,dtype='float32')\n",
    "            clip_data[trial][sensor]['features'] = pd.DataFrame(data=F,columns=features_list,dtype='float32')\n",
    "\n",
    "#     return clip_data #not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try making 74 feature list of acc and gyro feat\n",
    "def feature_extraction_accgyro(clip_data):\n",
    "    \"\"\"\n",
    "    Extract features from both sensors (accel and gyro) for current clips and trials\n",
    "    Input: dictionary of clips from each subject\n",
    "    Output: feature matrix from all clips from given subject and scores for each clip\n",
    "    \"\"\"\n",
    "    \n",
    "    features_list = ['RMSX','RMSY','RMSZ','rangeX','rangeY','rangeZ','meanX','meanY','meanZ','varX','varY','varZ',\n",
    "                    'skewX','skewY','skewZ','kurtX','kurtY','kurtZ','xcor_peakXY','xcorr_peakXZ','xcorr_peakYZ',\n",
    "                    'xcorr_lagXY','xcorr_lagXZ','xcorr_lagYZ','Dom_freq','Pdom_rel','PSD_mean','PSD_std','PSD_skew',\n",
    "                    'PSD_kur','jerk_mean','jerk_std','jerk_skew','jerk_kur','Sen_X','Sen_Y','Sen_Z']\n",
    "#                     ,'RMS_mag','range_mag',\n",
    "#                     'mean_mag','var_mag','skew_mag','kurt_mag','Sen_mag']\n",
    "    acclist = [s + '_acc' for s in features_list]\n",
    "    gyrlist = [s + '_gyr' for s in features_list]\n",
    "\n",
    "    for trial in clip_data.keys():\n",
    "\n",
    "        for sensor in clip_data[trial].keys():\n",
    "\n",
    "            #cycle through all clips for current trial and save dataframe of features for current trial and sensor\n",
    "            features = []\n",
    "            for c in range(len(clip_data[trial][sensor]['data'])):\n",
    "                rawdata = clip_data[trial][sensor]['data'][c]\n",
    "            \n",
    "                #acceleration magnitude\n",
    "                rawdata_wmag = rawdata.copy()\n",
    "                rawdata_wmag['Accel_Mag']=np.sqrt((rawdata**2).sum(axis=1))\n",
    "\n",
    "                #extract features on current clip\n",
    "\n",
    "                #Root mean square of signal on each axis\n",
    "                N = len(rawdata)\n",
    "                RMS = 1/N*np.sqrt(np.asarray(np.sum(rawdata**2,axis=0)))\n",
    "\n",
    "        #         RMS_mag = 1/N*np.sqrt(np.sum(rawdata_wmag['Accel_Mag']**2,axis=0))\n",
    "\n",
    "                #range on each axis\n",
    "                min_xyz = np.min(rawdata,axis=0)\n",
    "                max_xyz = np.max(rawdata,axis=0)\n",
    "                r = np.asarray(max_xyz-min_xyz)\n",
    "\n",
    "        #         r_mag = np.max(rawdata_wmag['Accel_Mag']) - np.min(rawdata_wmag['Accel_Mag'])\n",
    "\n",
    "                #Moments on each axis\n",
    "                mean = np.asarray(np.mean(rawdata,axis=0))\n",
    "                var = np.asarray(np.std(rawdata,axis=0))\n",
    "                sk = skew(rawdata)\n",
    "                kurt = kurtosis(rawdata)\n",
    "\n",
    "        #         mean_mag = np.mean(rawdata_wmag['Accel_Mag'])\n",
    "        #         var_mag = np.std(rawdata_wmag['Accel_Mag'])\n",
    "        #         sk_mag = skew(rawdata_wmag['Accel_Mag'])\n",
    "        #         kurt_mag = kurtosis(rawdata_wmag['Accel_Mag'])\n",
    "\n",
    "                #Cross-correlation between axes pairs\n",
    "                xcorr_xy = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,1],mode='same')\n",
    "                # xcorr_xy = xcorr_xy/np.abs(np.sum(xcorr_xy)) #normalize values\n",
    "                xcorr_peak_xy = np.max(xcorr_xy)\n",
    "                xcorr_lag_xy = (np.argmax(xcorr_xy))/len(xcorr_xy) #normalized lag\n",
    "\n",
    "                xcorr_xz = np.correlate(rawdata.iloc[:,0],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_xz = xcorr_xz/np.abs(np.sum(xcorr_xz)) #normalize values\n",
    "                xcorr_peak_xz = np.max(xcorr_xz)\n",
    "                xcorr_lag_xz = (np.argmax(xcorr_xz))/len(xcorr_xz)\n",
    "\n",
    "                xcorr_yz = np.correlate(rawdata.iloc[:,1],rawdata.iloc[:,2],mode='same')\n",
    "                # xcorr_yz = xcorr_yz/np.abs(np.sum(xcorr_yz)) #normalize values\n",
    "                xcorr_peak_yz = np.max(xcorr_yz)\n",
    "                xcorr_lag_yz = (np.argmax(xcorr_yz))/len(xcorr_yz)\n",
    "\n",
    "                #pack xcorr features\n",
    "                xcorr_peak = np.array([xcorr_peak_xy,xcorr_peak_xz,xcorr_peak_yz])\n",
    "                xcorr_lag = np.array([xcorr_lag_xy,xcorr_lag_xz,xcorr_lag_yz])\n",
    "\n",
    "                #Dominant freq and relative magnitude (on acc magnitude)\n",
    "                Pxx = power_spectra_welch(rawdata_wmag,fm=0,fM=10)\n",
    "                domfreq = np.asarray([Pxx.iloc[:,-1].idxmax()])\n",
    "                Pdom_rel = Pxx.loc[domfreq].iloc[:,-1].values/Pxx.iloc[:,-1].sum() #power at dominant freq rel to total\n",
    "\n",
    "                #moments of PSD\n",
    "                Pxx_moments = np.array([np.nanmean(Pxx.values),np.nanstd(Pxx.values),skew(Pxx.values),kurtosis(Pxx.values)])\n",
    "\n",
    "                #moments of jerk magnitude\n",
    "                jerk = rawdata_wmag['Accel_Mag'].diff().values\n",
    "                jerk_moments = np.array([np.nanmean(jerk),np.nanstd(jerk),skew(jerk[~np.isnan(jerk)]),kurtosis(jerk[~np.isnan(jerk)])])\n",
    "\n",
    "                #sample entropy raw data (magnitude) and FFT\n",
    "                sH_raw = []; sH_fft = []\n",
    "\n",
    "                for a in range(3):\n",
    "                    x = rawdata.iloc[:,a]\n",
    "                    n = len(x) #number of samples in clip\n",
    "                    Fs = np.mean(1/(np.diff(x.index)/1000)) #sampling rate in clip\n",
    "                    sH_raw.append(nolds.sampen(x)) #samp entr raw data\n",
    "                    #for now disable SH on fft\n",
    "                    # f,Pxx_den = welch(x,Fs,nperseg=min(256,n/4))\n",
    "                    # sH_fft.append(nolds.sampen(Pxx_den)) #samp entr fft\n",
    "\n",
    "                sH_mag = nolds.sampen(rawdata_wmag['Accel_Mag'])\n",
    "\n",
    "                #Assemble features in array\n",
    "        #         Y = np.array([RMS_mag,r_mag,mean_mag,var_mag,sk_mag,kurt_mag,sH_mag])\n",
    "                X = np.concatenate((RMS,r,mean,var,sk,kurt,xcorr_peak,xcorr_lag,domfreq,Pdom_rel,Pxx_moments,jerk_moments,sH_raw)) #,Y))\n",
    "                features.append(X)\n",
    "\n",
    "            F = np.asarray(features) #feature matrix for all clips from current trial\n",
    "        #     clip_data['features'] = pd.DataFrame(data=F,columns=features_list,dtype='float32')\n",
    "        \n",
    "            # add condition of sensor\n",
    "            if sensor == 'accel':\n",
    "                column_list=acclist\n",
    "                clip_data[trial]['features'] = pd.DataFrame(data=F,columns=column_list,dtype='float32')\n",
    "            else:\n",
    "                column_list=gyrlist\n",
    "                # Need to concat on columns\n",
    "                df_to_append = pd.DataFrame(data=F,columns=column_list,dtype='float32')\n",
    "                clip_data[trial]['features'] = pd.concat([clip_data[trial]['features'], df_to_append,ignore_index=True,axis=1)\n",
    "                \n",
    "                \n",
    "\n",
    "#     return clip_data #not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Pickle file dict\n",
    "subj = 'HC02'\n",
    "#f = open(os.path.join(dict_path, subj + 'dict.pkl'), 'rb')\n",
    "f = open(os.path.join(dict_path, subj + 'dict.pkl'), 'rb') # use for C: directory\n",
    "act_dict = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataAggregator_allLocs(tasks,clipsize=10000,overlap=0.9,locations=[],namesuffix=None,verbose=False):\n",
    "# omitted arg: task_scores_all\n",
    "    \"\"\"\n",
    "    Function: aggregate metadata and rawdata from all sensor (accel,gyro) locations\n",
    "    Rawdata is clipped into sliding windows, so incorporates code from gen_clips()\n",
    "    Metadata includes Subject, Trial, and Task\n",
    "    \n",
    "    startTS and endTS specificies fraction of raw sensors data to use to generate clips\n",
    "    \"\"\"\n",
    "\n",
    "    t0=time.time()\n",
    "#     #load subject scores (std motor assessments)\n",
    "#     mot_scores = pd.read_excel(os.path.join(scores_path, 'Scores.xls'))\n",
    "\n",
    "    #load subjects features data and assemble with scores/subj metadata\n",
    "    d = os.listdir(dict_path)\n",
    "    fnames = [filename[0:12] for filename in d if filename.startswith('HC')]\n",
    "    \n",
    "    ############################\n",
    "    #test only on 1st subject (TO DEBUG) which should be HC02\n",
    "    # skip HC01 - need to debug\n",
    "    # Comment out section to run all subjects\n",
    "    ############################\n",
    "#    fnames=[fnames[0]]\n",
    "    fnames = ['HC02dict.pkl']\n",
    "    print('\\nThe one subject we are testing on: ',fnames)\n",
    "\n",
    "    clip_data = pd.DataFrame() #the table with features and metadata\n",
    "\n",
    "    #loop through subjects\n",
    "    print(fnames)\n",
    "    for subj_filename in fnames:\n",
    "\n",
    "        #load current subject sensor data\n",
    "        subj = int(subj_filename[2:4]) #subj numerical code\n",
    "        f = open(os.path.join(dict_path,subj_filename),'rb')    \n",
    "        act_dict = pickle.load(f)\n",
    "        f.close()\n",
    "        print('\\nLoaded Subj %s sensor data'%subj)\n",
    "\n",
    "#         act_dict_left = gen_unimodal_data(act_dict_full, side='left')\n",
    "#         act_dict_right = gen_unimodal_data(act_dict_full, side='right')\n",
    "\n",
    "# Data structure: act_dict[task][trial][sensor]\n",
    "\n",
    "        #loop through tasks for current subject\n",
    "        for task in tasks: \n",
    "\n",
    "#             print('\\nTask is ', task)\n",
    "\n",
    "# discard side, so removed 1 indent\n",
    "#            for side in ['left','right']:\n",
    "\n",
    "            #select trials\n",
    "            Trials = act_dict[task].keys()\n",
    "            n_rec = len(Trials) #of sensor recordings\n",
    "\n",
    "######################\n",
    "# Note: gen_clips function starts around here...\n",
    "######################\n",
    "            #****EXTRACT RAW CLIPS FROM ALL LOCATIONS FOR CURRENT TRIAL AND APPEND***\n",
    "            for trial in Trials:\n",
    "#                 print('\\nTrial is ', trial)\n",
    "\n",
    "                clip_data_trial=pd.DataFrame() #storing all clips from current trial and side\n",
    "\n",
    "                locs = act_dict[task][trial].keys()\n",
    "\n",
    "                for location in locs:\n",
    "#                     print('\\nLocation is ', location)\n",
    "\n",
    "                    for s in ['accel','gyro']:\n",
    "#                         print('\\nSensor is ', s)\n",
    "\n",
    "                        if verbose:\n",
    "                            print(task,' sensortype = %s - trial %d'%(s,trial))\n",
    "                        #create clips and store in a list\n",
    "                        rawdata = act_dict[task][trial][location][s].copy()\n",
    "                        if rawdata.empty is True: #skip if no data for current sensor\n",
    "                            continue\n",
    "                        #reindex time (relative to start)\n",
    "                        idx = rawdata.index\n",
    "                        idx = idx-idx[0]\n",
    "                        rawdata.index = idx\n",
    "                        #create clips data\n",
    "                        deltat = np.median(np.diff(rawdata.index))\n",
    "                        clips = pd.DataFrame()\n",
    "\n",
    "#################################\n",
    "# Fine above\n",
    "# WOrk on below\n",
    "                        #take clips\n",
    "#                             location_name = re.sub(side, '', location) #column name dataframe (remove side info)\n",
    "                        # kept side for location name\n",
    "                        location_name = location\n",
    "\n",
    "                        idx = np.arange(0,rawdata.index[-1],clipsize*(1-overlap))\n",
    "                        for i in idx:\n",
    "                            c = (rawdata[(rawdata.index>=i) & (rawdata.index<i+clipsize)]).copy()\n",
    "                            #add time information to clips\n",
    "                            c['Time']=(c.index.values/1000)-(c.index.values/1000)[0]\n",
    "#                             df = pd.DataFrame({location_name+'_'+s:[c.values]},index=[trial])\n",
    "                            ##############\n",
    "                            # Changed dataframe structure to parse our location, sensor, and raw data in unstacked form\n",
    "                            df = pd.DataFrame({'location':location_name,'sensor':s,'rawdata':[c.values]})\n",
    "                            clips=pd.concat((clips,df)) #concatenate clips from current trial\n",
    "\n",
    "#                         ## keep this chunk and evaluate further if statements print\n",
    "#                         nc = len(clips)\n",
    "#                         if (nc < clip_data_trial.shape[0]) & (clip_data_trial.empty is False):\n",
    "#                             print(trial,location+s,len(clips))\n",
    "#                             nd = clip_data_trial.shape[0]-nc\n",
    "#                             print('current sensor has less clips - adding %d empties'%nd)\n",
    "#                             clips=pd.concat((clips,pd.DataFrame({location_name+'_'+s:np.empty(nd)},\n",
    "#                                                                 index=[np.ones(nd)*trial])))\n",
    "#                         if (nc > clip_data_trial.shape[0]) & (clip_data_trial.empty is False):\n",
    "#                             print(trial,location+s,len(clips))\n",
    "#                             nd = nc-clip_data_trial.shape[0]\n",
    "#                             print('current sensor has more clips - discarding last %d clips'%nd)\n",
    "#                             clips = clips.iloc[:-nd]\n",
    "#                         # end of chunk #######\n",
    "\n",
    "                        # may discard\n",
    "#                         clips.reset_index(inplace=True, drop=True)  # temp fix unique indexing bug from gen_unimodal_data\n",
    "#                                                                     # will look into, added 3/19/2018\n",
    "\n",
    "                        clip_data_trial=pd.concat((clip_data_trial,clips),axis=1) #concatenate clips across locations\n",
    "\n",
    "                #concatenates metadata to all clips for current trial\n",
    "                clip_data_trial['Subject']=subj\n",
    "                clip_data_trial['Task']=task\n",
    "#                 clip_data_trial['Side']=side\n",
    "                clip_data_trial['Trial']=trial #subj_score.iloc[visit]['visit']\n",
    "\n",
    "                ####### error here ############\n",
    "                clip_data = pd.concat((clip_data,clip_data_trial)) #concatenate clips across visits (index) \n",
    "# end of removed indent\n",
    "\n",
    "    #SAVING DATA \n",
    "    metadata = ['Subject','Trial','Task']\n",
    "    cols = clip_data.columns.tolist()\n",
    "    cols = np.setdiff1d(cols,metadata).tolist()\n",
    "    clip_data = clip_data[metadata+cols]\n",
    "    print('\\nClip Data matrix generated')\n",
    "    print(clip_data.shape)\n",
    "\n",
    "    if namesuffix == None:\n",
    "        namesuffix=''\n",
    "    saved_filename = 'DataRaw_AllLocs'+namesuffix+'.hdf5'\n",
    "    clip_data.to_hdf(os.path.join(features_path,saved_filename),'w')\n",
    "    print('Data matrix saved in ' + str(os.path.join(features_path,saved_filename)))\n",
    "    t = time.time()\n",
    "    eltime = (t-t0)/60\n",
    "    print('Elapsed time = %.2f min'%eltime)\n",
    "\n",
    "    return clip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
